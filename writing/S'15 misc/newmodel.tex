\section{Jointly learning syntactic parsing and semantic composition}

\subsection{Why joint learning?}

% tree and sequences have both been competitive, tree structures useful in some domains, but in many cases they are superceded by strong sequence models.

% our hypothesis: (i) natural language has compositional structure that can be exploited by representation learning models (ii) there is no guarantee that standard syntactic formalisms are especially well suited for this purpose

% new model, jointly learn syn and sem

\subsection{Model definition}

\begin{figure}[tp]
  \centering\small
 	\input{lattice_fig.tex}

        \caption{The connection structure of a LatticeNN. The correct binary parse tree for the example sentence, projected onto a lattice with the left-first rule, is shown in the bolded arrows.}
  \label{lattice-fig1}
\end{figure}

To compute the vector representation for a sentence, we populate the bottom row of the structure $\vec{y}^{(N,1...N)}$ with the embedddings of each of the words. We then compute the representations of each row starting at the second row from the bottom ($N - 1$), and moving from left to right. Computing one feature vector requires two steps. In the first step, we produce a distribution $\vec{o}$ over the operations \{\textsc{copy-L, copy-R, compose}\}, and in the second step, that distribution is used to compute a feature vector $\vec{y}$.

At each row $i$, we begin by computing a scalar score $s$ for each node $(i, j)$ representing how appropriate it would be to compose the two child nodes $(i+1, j)$ and $(i+1, j+1)$ at $(i, j)$. This decision is made primarily on the basis of the two child nodes, and a $C$-word window of context around them that is meant to allow the model to do a limited amount of ambiguity resolution. When the context window extends off the edge of the lattice in either direction, a single learned padding vector is used. % TODO
Including $\frac{j}{i}$ as an additional feature here is intended to allow the model to learn to use linear order in making its scoring decisions, allowing it to prefer to compose pairs of constituens that are closer to the left edge of the tree when all else is equal, and encouraging the model to conform to our left-first rule.

\begin{equation}
\label{scoringeqn}
s^{(i,j)} = \vec{v}_s \colvec{7}
	{\vec{y}^{(i + 1, j - C + 1)}}
	{...}
	{\vec{y}^{(i + 1, j)}}
	{\vec{y}^{(i + 1, j + 1)}}
	{...}
	{\vec{y}^{(i + 1, j + C)}}
	{\frac{j}{i}}
	 + b_s \\
\end{equation}
%
Since we wish for the model to compose only one pair of child nodes at each row $i$, we apply a softmax function to the full vector of scores computed at each row, and use the resulting values (which sum to one) to decide which nodes to compose.
%
\begin{equation} 
o^{(i,\,...)}_{\textsc{compose}} = \text{softmax}(\colvec{4}
	{s^{(i, 1)}}
	{s^{(i, 2)}}
	{...}
	{s^{(i, i)}}	
	) \\
\end{equation}
%
At training time, optional secondary parsing supervision can be used in the form of a label $\kappa^{(i)}$ indicating the location of the merge at layer $i$ of the left-first projection of the reference parse.

Then, for each node in the current row, we use these modified scores to compute another set of values summing to one, this time representing the fraction of the activation of the current node that should be drawn from each of the three operations:
%
\begin{gather}
o^{(i, j)}_{\textsc{copy-L}} = \sum_{x = j + 1}^{i} o^{(i,x)}_{\textsc{compose}} \\
o^{(i, j)}_{\textsc{copy-R}} = \sum_{x = 1}^{j - 1} o^{(i,x)}_{\textsc{compose}}
\end{gather}
%
The use of the sum for the two \textsc{copy} weights reflects the intuition that if a pair of nodes will be composed to the left of the current node, the current node must \textsc{copy-R}, and vice versa, in order to preserve the left-first tree projection structure. 

% TODO: What guarantees can we make about how much of the activation from each layer will be propagated to the next layer?

Finally, using these three computed weights, we compute the activations for the current node, using a standard RNN activation function for the \textsc{compose} operation:
% 

\begin{eqnarray}
&& \vec{y}^{(i,j)} =\\
&&\nonumber  o^{(i, j)}_{\textsc{copy-L}} \circ \vec{y}^{(i + 1, j)} +\\
&&\nonumber o^{(i, j)}_{\textsc{copy-R}} \circ \vec{y}^{(i + 1, j + 1)} +\\
&&\nonumber o^{(i, j)}_{\textsc{compose}} \circ \tanh(\mathbf{M_y} \colvec{2}{\vec{y}^{(i + 1, j)}}{\vec{y}^{(i + 1, j + 1)}} + \vec{b}_y)
\end{eqnarray}
%
Once all of the activations in the lattice have been computed, the top activation $\vec{y}^{(1,1)}$ can be treated as the embedding vector for the full sentence. For the sentiment classification task we study here, we pass it into a softmax classifier layer to predcit the probability of each sentiment label:
%
\begin{equation}
\vec{l} = \text{softmax}(\mathbf{M_l}\,\vec{y}^{(1, 1)} + \vec{b}_l)
\end{equation}
%
The loss for the example is then defined as the sum of three parts, a log likelihood term for the predicted sentiment class label, a sum of log likelihood terms for the correct composition location decisions (scaled by the number of ``middle'' layers in the lattice for which a composition location choice must be made), and an L2 regularization term. For sentiment relation label $\rho$, composition loctation label vector $\vec{\kappa}$, concatenated parameter vector $\theta$, and regularization strength hyperparameter $\lambda$ the loss is:
%

\begin{eqnarray}
loss & = & \lambda\theta^2 - \ln(l_\rho) -\\
\nonumber&&\frac{1}{N - 1} \sum_{i = 1}^{N - 2} \ln(o^{(i,\kappa^{(i)})}_{\textsc{compose}})
\end{eqnarray}
%
Parameter gradients can then be computed using standard backpropagation through structure \cite{goller1996learning}. Notably, when the composition location labels $\vec{\kappa}$, are used, the corresponding error signal at each row is used not only to train the scoring parameters $\vec{v}_s$, but also propagates error through the input vector used in eq. \ref{scoringeqn} and into the rest of the model.

Batching

% Possible supervision backoff proposal