\section{Introduction}\label{sec:intro}

Neural network models that encode sentences as vectors have been successfully used on a wide array of NLP tasks, including \todo{citations}. Many of the most successful of these models fall into one of two categories: sequence models based on recurrent neural networks build representations incrementally from left to right *CITE, while tree-structured models based on \ii{recursive} neural networks *CITE build representations incrementally according the hierarchical sturcture of linguistic phrases. 

Tree-structured models are presented as the more principled choice, since they directly exploit the principle of compositionality---fundamental to linguistic semantic theory \cite{Partee84,Janssen97}---that the meanings of phrases and sentences are constructed recursively from the meanings of their parts in a specific syntactically-guided order.

While both classes of model can perform objectively well on many tasks, and both are under active development, tree-structured models have not shown the kinds of dramatic performance improvements over sequence models that their billing would lead one to expect: head-to-head comparisons with sequence models show either modest improvements \todo{Cite TreeLSTM paper, and ...} or none at all \todo{cite Jiwei}. 

In this paper, we use focused artificial data experiments to effectively expose a possible explanation for these underwhelming results: standard sequence models can learn to implicitly exploit recursive compositional structure.

In particular, our aim is to test standard tree and sequence models on their handling of recursive structure by training each model on sentences whose length and recursion depth are limited, and then testing each on longer and more complex sentences, such that only a model that recognizes and exploits the recursive structure will be able to succeed. An existing experimental design meets this criterion nicely:\newcite{Bowman:Potts:Manning:2014} describe an experiment and corresponding dataset which tests the ability of two types of tree-structured neural network to learn to interpret (by supplying entailment judgments) statements of a simple propositional logic-based language, with a focus on evaluating the models on expressions more complex than those used in training. We propose a way to adapt the experiment presented in that work to sequence models and use it to compare an LSTM-based model with the tree-structured models in that paper, and we extend the experiment by testing multiple different sizes of training set in order to better understand how efficient each model is at learning the underlying generalizations.

As in \newcite{Bowman:Potts:Manning:2014}, we find that both TreeRNNs and TreeRNTNs are able to learn the necessary generalization, with their performance decaying gradually as the structures in the test set grow in size. Perhaps unsurprisingly, we also find that extending the training set to include larger structures mitigates this decay. More importantly, though, we find that a sequence model centered on a single-layer LSTM RNN is also able to generalize to unseen large structures, but that it only does this when trained on a larger and more complex training set than is needed by the tree-structured models. We take these results as evidence that the similar performance of tree and sequence models can be attributed to their exploitation of similar structures, and that there is value in attempting to further develop this ability in sequence models without fully sacrificing the flexibility that makes these models succeed.

\subsection{Related work}

TODO CITE JIWEI performs head-to-head comparisons of tree and sequence models on five NLP tasks over small to moderately-sized data sets and find that tree-structured models robustly outperform sequences on only one, concluding that sequence models are largely adequate for NLP. This paper aims to explore that result, specifically focusing on the question of whether compositional structure (implicit or explicit) can be dismissed as unessential in research on representation learning for standard NLP tasks.

Recent results on parsing by \todo{Cite Grammar as a Foreign Lg, Dyer} hints the outcome that we find. The simplest model presented in TODO CITE GaaFL encodes each sentence as a vector, and then generates a linearized parse (a sequence of brackets and constituent labels) for the sentence using only the information present in the vector. This suggests that the intermediate vector representation contains a tree-structured representation, and that that representation was constructed incrementally by the sequence model. However, the massive size of the training set that was necessary to train that model---250M tokens---makes an alternate explanation possible: the model could have primarily learned to simply recognize and generate only tree-structures that it had already seen, representing those structures as simple hashes rather than structured objects. If the model were able to map unseen sentences to familiar structures using these hashed representations, it could plausibly have achieved the strong results shown in that paper without exploiting the underlying tree structure of the linearized parses in a way that would generalize to substantially new structures.
