\section{Introduction}\label{sec:intro}

Neural network models that encode sentences as vectors have been successfully used on a wide array of NLP tasks, including \todo{citations}. Many of the most successful of these models fall into one of two categories: sequence models based on recurrent neural networks build representations incrementally from left to right *CITE, while tree-structured models based on \ii{recursive} neural networks *CITE build representations incrementally according the hierarchical sturcture of linguistic phrases. 

Tree-structured models are presented as the more principled choice, since they directly exploit the principle of compositionality---fundamental to linguistic semantic theory \cite{Partee84,Janssen97}---that the meanings of phrases and sentences are constructed recursively from the meanings of their parts in a specific syntactically-guided order.

While both classes of model can perform objectively well on many tasks, and both are under active development, tree-structured models have not shown the kinds of dramatic performance improvements over sequence models that their billing would lead one to expect. Even when the engineering hurdles around building tree-structured models are dealt with, head-to-head comparisons with sequence models show either modest improvements \todo{Cite TreeLSTM paper, and ...} or none at all \todo{cite Jiwei}. 

\todo{We use focused artificial data set to effectively demonstrate...}
We argue that this question is best studied through focused artificial data experiments, rather than through the inspection of existing learned models. There is no theory of distributed representation that is sufficiently detailed to allow us to look for the presence of tree-structured representations in model parameters or activations, and performing in-domain error analysis on trained models is unlikely to provide a sufficiently clear picture. An artificial data experiment, in contrast, allows us to single out the behavior that we want to see and evaluate a model's ability to learn it under ideal circumstances with ample clean data, such that if the is able to perform that behavior in any setting, it will do so in our experiment.

In particular, we aim to single out the behavior of learning to interpret sentences that use recursive structure. We specifically wish to test the recursion aspect of interpretation by training the model on sentences whose length and recursion depth are limited, and testing it on longer and more complex sentences, such that only a model that recognizes and exploits the recursive structure will be able to succeed. An existing experimental design meets this criterion nicely: \newcite{Bowman:Potts:Manning:2014} describe an experiment and corresponding dataset which tests the ability of two types of tree-structured neural network to learn to interpret (by supplying entailment judgments) statements of a simple propositional logic-based language, with a focus on evaluating the models on expressions more complex than those used in training. We propose a way to adapt the experiment presented in that work to sequence models and use it to compare an LSTM-based model with the tree-structured models in that paper, and we extend the experiment by testing multiple different sizes of training set in order to better understand how efficient each model is at learning the underlying generalizations.

As in \newcite{Bowman:Potts:Manning:2014}, we find that both TreeRNNs and TreeRNTNs are able to learn the necessary generalization, with their performance decaying gradually as the structures in the test set grow in size. Perhaps unsurprisingly, we also find that extending the training set to include larger structures mitigates this decay. More importantly, though, we find that a sequence model centered on a single-layer LSTM RNN is also able to generalize to unseen large structures, but that it only does this when trained on a larger and more complex training set than is needed by the tree-structured models. We take these results as evidence that sequence models are capable of learning tree-structured representations when trained on a task that demands them. \todo{higher level conclusions based on the two interpretations from above}

\subsection{Related work}

TODO CITE JIWEI performs head-to-head comparisons of tree and sequence models on five NLP tasks over small to moderately-sized data sets and find that tree-structured models robustly outperform sequences on only one, concluding that sequence models are largely adequate for NLP. This paper aims to explore that result, specifically focusing on the question of whether compositional structure (implicit or explicit) can be dismissed as unnecessary in research on representation learning for standard NLP tasks.

Recent results on parsing by \todo{Cite Grammar as a Foreign Lg, Dyer} hints the outcome that we find. The simplest model presented in TODO CITE GaaFL encodes each sentence as a vector, and then generates a linearized parse (a sequence of brackets and constituent labels) for the sentence using only the information present in the vector. This suggests that the intermediate vector representation contains a tree-structured representation, and that that representation was constructed incrementally by the sequence model. However, the massive size of the training set that was necessary to train that model---250M tokens---makes an alternate explanation possible: the model could have primarily learned to simply recognize and generate only tree-structures that it had already seen, representing those structures as simple hashes rather than structured objects. If the model were able to map unseen sentences to familiar structures using these hashed representations, it could plausibly have achieved the strong results shown in that paper without exploiting the underlying tree structure of the linearized parses in a way that would generalize to substantially new structures.
