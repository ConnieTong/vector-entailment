\section{Introduction}\label{sec:intro}

Neural network models that encode sentences as vectors have been successfully used in a wide array of NLP tasks, including translation \cite{sutskever2014sequence}, parsing \cite{dyer2015transition}, and sentiment analysis \cite{tai2015improved}. Many of the most successful of these models are based on either tree models or sequence models. sequence models based on recurrent neural networks build representations incrementally from left to right \cite{elman1990finding,sutskever2014sequence}, while tree models based on \ii{recursive} neural networks \cite{goller1996learning,socher2011semi} build representations incrementally according the hierarchical sturcture of linguistic phrases.

Tree models are presented as the more principled choice, since they directly exploit the principle of compositionality---a fundamental idea in linguistic semantics \cite{Partee84,Janssen97}---which states that the meanings of phrases and sentences are constructed recursively from the meanings of their parts in a specific syntactically-guided order.

While both classes of model can perform objectively well on many tasks, and both are under active development, tree models have not shown the kinds of dramatic performance improvements over sequence models that their billing would lead one to expect: head-to-head comparisons with sequence models show either modest improvements \cite{tai2015improved} or none at all \cite{li2015tree}. 
In this paper, we use focused artificial data experiments to expose a possible explanation for these results: standard sequence models can learn to implicitly exploit recursive compositional structure.

In particular, we aim test standard tree and sequence models on their handling of recursive structure by training the models on sentences whose length and recursion depth are limited, and then testing them on longer and more complex sentences, such that only models that exploit the recursive structure will be able to succeed. An existing experimental design meets this criterion: \newcite{Bowman:Potts:Manning:2014} describe an experiment and corresponding dataset which tests the ability of two tree models to learn to interpret statements of a simple artificial language. We propose a way to adapt than experiment to sequence models involving decorating the statements with an explicit bracketing, and use the experiment to compare an LSTM sequence model with the three treee models, with a focus on what data each model needs in order to learn the needed generalizations.

As in \newcite{Bowman:Potts:Manning:2014}, we find that standard tree neural networks are able to learn the necessary generalization, with their performance decaying gradually as the structures in the test set grow in size. We also find that extending the training set to include larger structures mitigates this decay. In addition, we find that a sequence model centered on a single-layer LSTM is also able to generalize to unseen large structures, but that it only does this when trained on a larger and more complex training set than is needed by the tree models. 

\paragraph{Related work}
\newcite{li2015tree} performs head-to-head comparisons of tree and sequence models on five NLP tasks over small to moderately-sized data sets and find that tree models robustly outperform sequences on only one, concluding that ``recurrent models are sufficient to capture the compositional semantics required for many NLP tasks.''

Recent successes on parsing by \newcite{vinyals2014grammar} and \newcite{dyer2015transition} hint that sequence models may be able to exploit recursive structure. The simplest model presented in Vinyals et al.~uses an LSTM sequence model to encode each sentence as a vector, and then generates a linearized parse (a sequence of brackets and constituent labels) using only the information present in the vector, suggesting that the LSTM was able to encode the tree. However, the massive size of the training set that was necessary to train that model---250M tokens---leaves open the possibility that the model primarily learned to recognize and generate only tree structures that it had already seen, representing those structures as simple hashes rather than structured objects. If the model were able to map unseen sentences to familiar structures using these hashed representations, it could plausibly have achieved the strong results shown in that paper without showing demonstrating an ability to represent recursive structure, at least in a way that would generalize to substantially new structures. Our paper aims to understand whether this ability to generalize is likely or possible.
