\section{Results and discussion}\label{sec:discussion}

The results are shown in Figure \ref{prop-results}. All three models were able to largely memorize the training data (TODO, TODO, and TODO for $\le$6). With sufficient training data (in the $\le$6 setting), the two tree models were able to perform at above 99\% (TODO) on examples of size $\le$2 with a steady decay in performance continuing through TODO\% at size 12, and similar, if steeper, decays with smaller training sets.

The LSTM performs fairly poorly in the $\le3$ setting, with performance at 3 a mediorce TODO\%, and an abrubt drop to TODO\% at 4, suggesting that the model did not acquire the needed generalizations. With more ample training data in the $\le6$ condition, though, the LSTM shows fairly good accuracy on short examples, and a smooth decay with no abrupt drop. 

\paragraph{Conclusion}

We find that 
