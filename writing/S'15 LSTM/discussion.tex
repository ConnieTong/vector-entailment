\section{Results and discussion}\label{sec:discussion}

The results are shown in Figure \ref{prop-results}. All three models were able to largely memorize the training data (TODO, TODO, and TODO for $\le$6). With sufficient training data (in the $\le$6 setting), the two tree models were able to perform at above 99\% (TODO) on examples of size $\le$2 with a steady decay in performance continuing through TODO\% at size 12, and similar, if steeper, decays with smaller training sets. These robustly beat the simple baselines reported in Bowman et al.: the most frequent class occurs just over 50\% of the time, and a summing model that ignores word order does reasonably on the shortest examples but falls below 60\% by bin 4.

The LSTM performs fairly poorly in the $\le3$ setting, with performance at 3 a mediorce TODO\%, and an abrubt drop to TODO\% at 4, suggesting that the model did not acquire the needed generalizations. With more ample training data in the $\le6$ condition, though, the LSTM shows fairly good accuracy on short examples, and a smooth decay with no abrupt drop. 

\paragraph{Conclusion}

We find that all three \todo{Still using three?} models can exploit recursive structure to interpret sentences with complex unseen structures, and that tree structured model's biases allow them to learn to do this more effectively from less data. 
We interpret these results as evidence that the similar performance of tree and sequence models can be attributed to their fundamentally similar representational systems, with both models recognizing compositional structure when it is present, the former doing so more reliably, and the latter benefiting elsewhere---either because of architectural choices that make backpropagation more effective, or because interpreting sentences from left to right allows the model to better simulate other aspects of human language comprehension. We finally suggest that there is value in attempting to further develop this ability in sequence models without fully sacrificing the flexibility that makes these models succeed.
