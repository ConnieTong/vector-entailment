\section{Testing sentence models on entailment} \label{methods}

\begin{figure}[t]
  \centering
  \input{figure1}
  \caption{In our model, two copies of a sentence model---based on either tree (b) or sequence (c) models---encode the two sentences. A multilayer classifier component (a) then uses the resulting vectors to predict a label that reflects the logical relationship between the two sentences.}
  \label{sample-figure}
\end{figure}

\begin{figure*}[t]
  \centering
  \begin{subfigure}[t]{0.04\textwidth}
      \includegraphics[height=1.4in]{scale.pdf}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
  \includegraphics[height=1.4in]{fig3c.pdf}
  \caption{Training on sz. $\le$3.}
  \end{subfigure}~~~~
\begin{subfigure}[t]{0.24\textwidth}
    \includegraphics[height=1.4in]{fig4c.pdf}
  \caption{Training on sz. $\le$4.}
  \end{subfigure}~~~~
\begin{subfigure}[t]{0.24\textwidth}
      \includegraphics[height=1.4in]{fig6c.pdf}
  \caption{Training on sz. $\le$6.}
\end{subfigure}~~
\begin{subfigure}[t]{0.08\textwidth}
      \includegraphics[height=1.4in]{leg.pdf}
\end{subfigure}
  \caption{Test accuracy on three experiments with increasingly rich training sets. The horizontal axis on each graph divides the test set expression pairs into bins by the number of logical operators in the more complex of the two expressions in the pair. The dotted line shows the maximum size in each training set.}
  \label{prop-results} 
\end{figure*}

We use the architecture depicted in Figure~\ref{fig:model:top}, which builds on the one used in \newcite{Bowman:Potts:Manning:2014}. The model architecture uses two copies of a single sentence model (a tree or sequence model) to encode the premise and hypothesis (left and right side) sentences, and then uses those encodings as the features for a multilayer classifier which predicts one of the seven relations. Since the encodings are computed separately, the sentence models are forced to encode every element of the sentence meaning that the downstream model will need.

\paragraph{Classifier}
The classifier component of the model consists of a combining layer which takes the two sentence representations as inputs, followed by two neural network layers, then a softmax classifier.
For the combining layer, we use a neural tensor network (NTN, \citealt{chen2013learning}) layer, which sums the output of a plain recursive/recurrent neural network layer with a vector computed using two multiplications with a learned (full rank) third-order tensor parameter:
\begin{gather} 
\label{TreeRNN}
\vec{y}_{\textit{NN}} = \tanh(\mathbf{M} \colvec{2}{\vec{x}^{(l)}}{\vec{x}^{(r)}} + \vec{b}\,) \\
\label{TreeRNTN} 
\vec{y}_{\textit{NTN}} = \vec{y}_{\textit{NN}} + \tanh(\vec{x}^{(l)T} \mathbf{T}^{[1 \ldots n]} \vec{x}^{(r)})
\end{gather} 

Our model is largely identical to the Bowman et al.~model, but adds the two additional $\tanh$ NN layers, which we found help performance across the board, and also uses the NTN combination layer when evaluating all three models, rather than just the TreeRNTN model, so as to ensure that the three sentence models are compared in as similar a setting as possible.

\paragraph{Sentence models}
The sentence encoding component of the model transforms the (jointly learned) embeddings of the input words into a single sentence vector. We experiment with tree models (Figure~\ref{fig:model:tree}) with TreeRNN (eqn.~\ref{TreeRNN}), TreeRNTN (eqn.~\ref{TreeRNTN}), and TreeLSTM \cite{tai2015improved} activation functions. In addition, we use a sequence model (Figure~\ref{fig:model:seq}) with an LSTM activation function \cite{hochreiter1997long} implemented as in \newcite{zaremba2015recurrent}. Experiments with simpler non-LSTM RNN sequence models tended to badly underfit the training data, and are not included here.

\paragraph{Training} We randomly initialize all embeddings and layer parameters, and train them using minibatch stochastic gradient descent with AdaDelta \cite{zeiler2012adadelta} learning rates. Our objective is the standard negative log likelihood classification objective with L2 regularization (tuned on a separate train/test split). All models were trained for 100 epochs, after which all had largely converged without significantly declining from their peak performances.\todo{Replot learning curve, resummarize numbers.}
