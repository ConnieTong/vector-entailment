\section{Sentence embedding models for NLI} \label{methods}
\todo{Rewrite in a shorter form, add LSTMs}

\begin{figure}[t!]
  \centering
  \input{figure1}
  \caption{In our model, two separate sentence models---based on either tree (b) or sequence (c) models---build up vector representations for each of two sentences. A multilayer classifier component (a) then uses the resulting vectors to predict a label that reflects the logical relationship between the two sentences.} 
  \label{sample-figure}
\end{figure}

We use the model architecture depicted in Figure \ref{fig:model:top}. This approach is loosely based on the model used in the experiments by \newcite{Bowman:Potts:Manning:2014}. The model architecture uses two copies of a single sentence embedding network---a TreeRNN, TreeRNTN, or LSTM RNN, depending on the precise experiment---to embed the premise and hypothesis sentences, and then uses those embeddings as the features for a multilayer classifier which predicts one of the seven relations. Since the embeddings are computed separately, the embedding models are forced encode every element of the sentence meaning that the downstream model will need. Though this prevents us from exploiting cross-sentence alignment information that could allow for the learning of more effective entailment models, it allows us to focus our evaluation on models that capture the meanings of single sentences.

\paragraph{Classifier}
The classifier component of the model consists of a combinging layer which takes the two sentence representations as inputs, followed by two neural network layers, then a softmax classifier.
For the combining layer, we use a (recursive) neural tensor network ((R)NTN, \citealt{chen2013learning}) layer, which sums the output of a plain recursive/recurrent neural network layer with a vector computed using two multiplications with a learned (full rank) third-order tensor parameter:
\begin{gather} 
\label{TreeRNN}
\vec{y}_{\textit{RNN}} = \tanh(\mathbf{M} \colvec{2}{\vec{x}^{(l)}}{\vec{x}^{(r)}} + \vec{b}\,) \\
\label{TreeRNTN} 
\vec{y}_{\textit{RNTN}} = \vec{y}_{\textit{RNN}} + \tanh(\vec{x}^{(l)T} \mathbf{T}^{[1 \ldots n]} \vec{x}^{(r)})
\end{gather} 
Our model is largely identical to the Bowman et al. model, but adds the two additional $\tanh$ NN layers, which we found help performance across the board, and also uses the NTN combination layer when evaluating all three models, rather than just the TreeRNTN model, so as to ensure that the three sentence embedding models are compared in as similar a setting as possible.


\paragraph{Sentence embedding}
The sentence embedding component of the model transforms the (jointly learned) embeddings of the input words into a single sentence vector. We experiment with two tree-structured models (Figure \ref{fig:model:tree}), one with an RNTN activation function or a plain RNN activation function. Both are structured using the parses that are provided with the data. In addition, we use a sequence model (Figure \ref{fig:model:seq}) with an LSTM activation function \cite{hochreiter1997long} with the precise formulation from \newcite{zaremba2015recurrent}.

We additionally experimented with a plain RNN---the sequence equivalent of the TreeRNN---but were not able to coerce that weaker model into learning to model even the training data well, so they are omitted below. LSTM, in contrast are the most widely used of a class of models that attempt to avoid the extreme sensitivity to initialization and gradient magnitude that make RNN training so unreliable.

\paragraph{Training} We randomly initialize all embeddings and layer parameters, and then train all parameters using minibatch stochastic gradient descent with AdaDelta \cite{zeiler2012adadelta} learning rates. Our objective is the standard negative log likelihood classification objective with L2 regularization. All models were stopped at 100 epochs (TODO), it which point all had largely converged without significantly declining from their peak performances.
