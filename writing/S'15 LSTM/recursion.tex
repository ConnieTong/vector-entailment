\section{The task}\label{sec:recursion}
\paragraph{Reasoning about entailment} 
The data that we use define a version of the recognizing textual entailment task, in which the goal is to determine what kind of logical consequence relation holds between two sentences, drawing on a small fixed vocabulary of relations such as entailment, contradiction, or synonymy. This task is well suited to evaluating neural network models for sentence interpretation: models must develop comprehensive represenations of the meanings of each sentence to do well at the task, but the data does not force these representations to take a specific form, allowing the model to learn whatever kind of representations it can use most effectively.

The data we use are labeled with the seven relation system of \newcite{maccartney2009extended}, depicted in Table \ref{b-table}.

\begin{table}[tp]
  \centering\small
  \renewcommand{\arraystretch}{1}
  \begin{tabular}{l c l l} 
    \toprule
    Name & Symb. & Set-theoretic definition \\ 
    \midrule
strict entailment         & $\natfor$   & $x \subset y$  \\ 
    strict rev. entailment & $\natrev$   & $x \supset y$  \\ 
    equivalence        & $\nateq$    & $x = y$   \\ 
    alternation        & $\natalt$   & $x \cap y = \emptyset \wedge x \cup y \neq \mathcal{D}$ \\ 
    negation           & $\natneg$   & $x \cap y = \emptyset \wedge x \cup y = \mathcal{D}$   \\
    cover              & $\natcov$   & $x \cap y \neq \emptyset \wedge x \cup y = \mathcal{D}$ \\ 
    independence       & $\natind$   & (else)\\
    \bottomrule
  \end{tabular}
  \caption{\label{b-table}The seven relations are defined abstractly on pairs of sets drawing from the universe $\mathcal{D}$, but can be straightforwardly applied to any pair of natural language words, phrases, or sentences. The relations are defined so as to be mutually exclusive. (table adapted from \protect\citealt{Bowman:Potts:Manning:2014})} %-%
\end{table}


\paragraph{The artificial language} The language described in Bowman et al. (\S4) is extremely impoverished: its vocabulary consists only of six unanalyzed word types ($p_1, p_2, p_3, p_4, p_5, p_6$), \word{and}, \word{or}, and \word{not}. Sentences of the language can be straightforwardly interpreted as statements of propositional logic (where the six unanalyzed words are variables), and labeled sentence pairs can be interpreted as theorems of that logic. Some example pairs are provided in Table \ref{tab:plexs}.

Crucially, the language is defined such that any sentence can be embedded under negation or conjunction to create a new sentence, allowing for arbitrary-depth recursion, and the scope of negation and conjunction are determined only by bracketing with parentheses (rather than bare word order). The compositional structure of each sentence can thus be an arbitrary tree structure, and interpreting a sentence correctly requires using that structure.

The data comes with parentheses representing complete binary bracketing. Our two models use this information differently. For the tree structured model, we assumble the tree structure for each sentence according to the parentheses, but do not treat them as word tokens. For the sequence model, we treat the parentheses as words, and learn their embeddings.

\begin{table}[tp]
  \centering\small
%  \begin{subtable}[t]{0.45\textwidth}
%    \centering
%    \begin{tabular}[t]{l l}
%      \toprule
%      Formula     & Interpretation \\
%      \midrule
%      $p_1$, $p_2$, $p_3$, $p_4$, $p_5$, $p_6$ & $\sem{x} \in \{\True, \False\}$ \\
%      $\plneg \varphi$ & $\True$ iff $\sem{\varphi} = \False$ \\
%      $(\varphi \pland \psi)$ & $\True$ iff $\False \notin \{\sem{\varphi}, \sem{\psi}\}$ \\
%      $(\varphi \plor \psi)$  & $\True$ iff $\True \in \{\sem{\varphi}, \sem{\psi}\}$ \\
%      \bottomrule
%    \end{tabular}    
%    \caption{Well-formed formulae. $\varphi$ and $\psi$
%      range over all well-formed formulae, and $\sem{\cdot}$ is
%      the interpretation function mapping formulae into $\{\True,
%      \False\}$.}\label{tab:pl}
%  \end{subtable}
    \begin{tabular}[t]{r c l}
      \toprule
      $\plneg p_3$        & $\natneg$ & $p_3$ \\
      $\plneg \plneg p_6$ & $\nateq$  & $p_6$ \\
      $p_3$               & $\natfor$ & $(p_3 \plor p_2)$ \\
      $(p_1 \plor (p_2 \plor p_4))$               & $\natrev$ & $(p_2 \pland  \plneg p_4)$ \\
      %$(a \natfor b)$   & $\nateq$  & $(b \natrev a)$ \\	
      $\plneg\, (\plneg p_1 \pland \plneg p_2)$ & $\nateq$ & $(p_1 \plor p_2)$ \\ 
      $(p_3 \pland \plneg p_1 ) \plor \plneg p_3$    & $\natrev$& $\plneg\, (p_3 \plor p_2)$ \\
      %<	( not ( c ( or b ) ) )	( ( c ( and ( not a ) ) ) ( or ( not c ) ) )
      \bottomrule
    \end{tabular}
    \caption{Examples of short expressions from the artificial data introduced by \protect\citealt{Bowman:Potts:Manning:2014}. For brevity, we only show the parentheses that are needed to disambiguate the sentences rather than the full binary bracketings.}\label{tab:plexs}
\end{table}

\paragraph{Corpus size and structure}
The sentence pairs in the corpus are divided into thirteen bins according to the number of logical connectives (\word{and, or, not}) in the longer of the two sentences in the pair (the data contain no sentences with more than 12 connectives). We test the model on each bin separately (58k total examples, using an 80/20\% train/test split) in order to evaluate how its performance depends on the complexity of the sentences. In three experiments, we train our models on the training portions of bins 0--3 (62k examples), bins 0--4 (90k), and bins 0--6 (160k). Capping the size of the training sentences below length 12 allows us to evaluate how the models interpret the sentences: if their performance falls off abruptly above the cutoff, it is reasonable to assume that the models are depending heavily on specific sentence structures, and cannot generalize to new structures. If then performance decays gradually\footnote{Since sentences are fixed-dimensional vectors of fixed-precision floating point numbers, the models will inevitably will start to make errors on sentences above a some length, and the (necessary) use of L2 regularization exacerbates this for sentences which are longer than those seen in training.} with no such abrupt change, then it must have learned a more generally valid interpretation function for the language which respects its recursive structure.



