\section{Introduction}\label{sec:intro}

Neural networks that encode sentences as real-valued vectors have been successfully used in a wide array of NLP tasks, including translation \cite{sutskever2014sequence}, parsing \cite{dyer2015transition}, and sentiment analysis \cite{tai2015improved}. 
These models are generally either 
sequence models based on recurrent neural networks, which build representations incrementally from left to right \cite{elman1990finding,sutskever2014sequence}, or tree models based on \ii{recursive} neural networks, which build representations incrementally 
according to the hierarchical structure of linguistic phrases \cite{goller1996learning,socher2011semi}. 

While both model classes perform
well on many tasks, and both are under active development,
tree models are often presented as the more principled choice, since they align with standard linguistic assumptions about constituent structure and the compositional derivation of complex meanings.
Nevertheless,
tree models have not shown the kinds of dramatic performance improvements over sequence models that their billing would lead one to expect: head-to-head comparisons with sequence models show either modest improvements \cite{tai2015improved} or none at all \cite{li2015tree}. 

We propose a possible explanation for these results: standard sequence models can learn to
exploit recursive syntactic structure in generating sentence meaning representations, thereby 
learning to use the 
structure that tree-structured models are explicitly designed around. This first requires that
sequence models are able to identify syntactic structure in natural language. We believe this is plausible, on the basis of other recent research \cite{vinyals2014grammar,Karpathy2015vaurn}.
In this paper, we  evaluate whether LSTM models are able to use that structure to guide interpretation, 
focusing on cases where the relevant syntactic structure is clearly indicated in the data.

We compare standard tree and sequence models on their handling of recursive structure by training the models on sentences whose length and recursion depth are limited, and then testing them on longer and more complex sentences, such that only models that exploit the recursive structure will be able to generalize in a way that yields correct interpretations for these test sentences. Our methods are based on  \newcite{Bowman:Potts:Manning:2014}, in which we describe an experiment and corresponding artificial dataset which tests this ability in two tree models. We adapt that experiment here to sequence models by decorating the statements with an explicit bracketing, and we use this design to compare an LSTM sequence model with three tree models, with a focus on what data each model needs in order to learn the needed generalizations.

As in Bowman et al., we find that standard tree neural networks are able to make the necessary generalizations, with their performance decaying gradually as the structures in the test set grow in size. We additionally find that extending the training set to include larger structures mitigates this decay. Then considering sequence models, we find that a single-layer LSTM is also able to generalize to unseen large structures, but that it does this only when trained on a larger and more complex training set than is needed by the tree models.

Our results engage with those of \newcite{vinyals2014grammar} and \newcite{dyer2015transition}, who find that sequence models can learn to recognize syntactic structure in natural language, at least when trained on explicitly syntactic tasks. The simplest model presented in Vinyals et al.~uses an LSTM sequence model to encode each sentence as a vector, and then generates a linearized parse (a sequence of brackets and constituent labels) with high accuracy using only the information present in the vector. This shows that the LSTM is able to identify the correct syntactic structures and also hints that it is able to develop a generalizable method for encoding these structures in vectors. However, the massive size of the dataset needed to train the model---250M tokens---leaves open the possibility that it primarily learns to generate only tree structures that it has already seen, representing them as simple hashes---which would not capture unseen tree structures---rather than as structured objects.
Our experiments, though, show that LSTMs can learn to understand tree structures when given enough data, suggesting that there is no fundamental obstacle to learning this kind of structured representation.