\section{A new corpus for natural language inference}\label{sec:discussion}

% TODO: Rewrite and expand motivation here (CP?)

Our ultimate aim in this work is to develop supervised models for sentence representation that can accurately capture natural language meaning. While sentiment tasks like SST have provided a useful testbed for sentence representation models, sentiment labeling only requires that models be able to encode a small piece of the full expressive capacity of language. We claim that the task of natural language inference (also called recognizing textual entailment, or RTE) is significantly more demanding, and that strong performance on this task is good evidence of a model's overall strength in sentence representation.

\subsection{Grounding with imagined images}

% TODO: Rewrite and expand (CP?)

Quote from SICK paper \cite{marelli2014sick}:

\begin{quote}
Not unreasonably, subjects found that, say, \ii{A woman is wearing an Egyptian headdress} does not contradict \ii{A woman is wearing an Indian headdress}, since one could easily imagine both sentences truthfully uttered to refer to a single scene where two different women are wearing different headdresses. In the future, a higher proportion of CONTRADICTION labels could be elicited by using grammatical and possibly visual cues (pictures) encouraging co-indexing of the entities in the two sentences.
\end{quote}

\subsection{Data collection}

We used Amazon Mechanical Turk for data collection. In each individual task, a worker was presented with premise scene descriptions from a preexisting corpus, and asked to supply hypotheses that fit each of our three labels (\ii{entailment}, \ii{neutral}, and \ii{contradiction}).

For the premises, we used the Flickr30k corpus \cite{hodoshimage}, a collection of approximately 160k captions from that corpus (corresponding to about 30k images) collected in an earlier crowdsourced effort. The captions were not authored by the photographers who took the source images, they tend to contain relatively literal scene descriptions that are suited to our approach, rather than those typically associated with personal photographs (as in their example: ``Our trip to the Olympic Peninsula''). We presented each caption to at least one worker, and some to more than one. With the exception of a handful that were flagged by workers as excessively difficult, at least three labeled pairs are available for each caption.

The instructions that we provided to the workers are shown in Fig.~\ref{instructions-1}. Below the instructions were three fields for each of three requested sentences, corresponding to our \ii{entailment}, \ii{neutral}, and \ii{contradiction} labels, a fourth field (marked optional) for reporting problems, and a link to an FAQ page. That FAQ grew over the course of data collection, but discussed disallowed techniques (e.g., reusing the same sentence for many different prompts, which we saw in a few cases), sentence length and complexity (we did not enforce a minimum length, and we allowed bare NPs as well as full sentences), and logistical issues around payment timing.

\begin{figure}
\footnotesize
% \toprule
The Stanford University NLP Group is collecting data for use in research on computer understanding of English. We appreciate your help!

We will show you the caption for a photo. We will not show you the photo. Using only the caption and what you know about the world:
\begin{itemize}
\item Write one alternate caption that is \textbf{definitely} a \textbf{true} description of the photo. \ii{Example: For the caption "Two dogs are running through a field." you could write "There are animals outdoors."}
\item Write one alternate caption that \textbf{might be} a \textbf{true} description of the photo. \ii{Example: For the caption "Two dogs are running through a field." you could write "There are animals outdoors."}
\item Write one alternate caption that is \textbf{definitely} a \textbf{false} description of the photo. \ii{Example: For the caption "Two dogs are running through a field." you could write "There are animals outdoors." This is different from the maybe correct category because its impossible for the dogs to be both running and sitting.}
\end{itemize}
\caption{\label{instructions-1}The instructions used on Mechanical Turk for data collection.}
\end{figure}

% TODO: Do we need statistics about the number/distribution of workers?
% TODO: Do we need statistics about how much of ImageFlickr we rejected/double-annotated?

\subsection{Data validation}

The majority of the sentence pairs in the corpus have not been reviewed by anyone since their construction as part of the data collection Mechanical Turk task, except to remove blank responses and responses some pairs which the original author flagged for review. However, in order to measure the quality of our corpus, and in order to construct maximally useful testing and development sets, we did perform an additional round of validation for about 54k (TODO) examples.

This validation phase is not fundamentally different from the Mechanical Turk labeling task used for the SICK entailment data: we present workers with pairs of sentences (in our case, in batches of five at a time), and ask them to choose a single label for each pair. We supplied each pair to four annonators, yielding five labels per pair including the label used by the original composer of the pair. The instructions were broadly similar to the instructions for initial data collection shown in Fig.~\ref{instructions-1}, and linked to a similar FAQ.

The results of this validation process are summarized in Fig.~\ref{validation-stats}. For each pair that we validated, we chose a gold label. If any one of the three labels was chosen by at least three of the five annotators, that label is the gold label. If there was no such consensus, which occurred in about 0.2\% of cases, we chose the label `-'. While these unlabeled examples are included in the corpus distribution, they are unlikely to be helpful for the core classification task of NLI, and we 

TODO: Data filtering? Data re-labeling?

\begin{table*}
  \centering\footnotesize
  \begin{tabular}{p{7.5cm}lp{5.5cm}}
  \toprule
A young man in a striped hoodie looks at an artistic Levi's advertisement that says "We are all workers." & \ii{entailment} & There is a man in a hoodie looking at an advertisement.\\
\rule{0pt}{3ex}Two men on laptops in a dark room. & \ii{entailment} & Two men are on their laptops.\\
\rule{0pt}{3ex}A naked baby and toddler smear each other in paint. & \ii{neutral} & The kids are in the bathtub.\\
\rule{0pt}{3ex}A woman is focusing on her game of curling. & \ii{neutral} & There is a women playing a game indoors\\
\rule{0pt}{3ex}A group of people dressed in green and white are dancing in a circle outside of a white building. & \ii{contradiction} & A group performs indoors.\\
\rule{0pt}{3ex}A balding professor giving a lecture to students on the topic of "opportunity". & \ii{neutral} & A female professor with hair giving a lecture to students.\\
% TODO: Replace with *validated* examples from the post-rc2 training set, include all 5 labels
    \bottomrule
% From 1.0rc2
  \end{tabular}
  \caption{\label{snli-examples}Randomly chosen examples from SNLI.}
\end{table*}

\subsection{The distributed corpus}

% Screenshot from phase 1


% Screenshot from phase 2

The data is freely available at (POST DATA), and is released under a CreativeCommons
Attribution-ShareAlike licence, which is the same licence used for the Flickr30k source captions.

\begin{figure}
\center
\includegraphics[width=3.05in]{length_dist}
    % From 1.0rc2
\caption{\label{b-table}The distribution of sentence length.} 
\end{figure}

\begin{table}
\center
  %\setlength{\tabcolsep}{15pt}
  %\renewcommand{\arraystretch}{1.2}
  \begin{tabular}{l l} 
    \toprule
Pairs in corpus & 570,157\\
Training pairs &  550,159\\
Dev pairs &  9,999\\
Test pairs &  9,999\\
\midrule
Premise mean token count & 14.1\\
Hypothesis mean token count & 8.3 \\
\midrule
Premise complete sentences & 70.2 ??\%\\
Hypothesis complete sentences & 75.4 ??\%\\
    \bottomrule
  \end{tabular}
    % From 1.0rc2
\caption{\label{validation-stats}Key statistics for the raw sentence pairs. Since the two halves of each pair were collected separately, we report statistics for both.} 
\end{table}

\begin{table}
\center
  %\setlength{\tabcolsep}{15pt}
  %\renewcommand{\arraystretch}{1.2}
  \begin{tabular}{l l} 
    \toprule
Validated pairs & 53,852\\
gold label $\ne$ original label & 8.5\%\\
Unanimous gold & 57.7\%\\
No gold (no 3 labels match) & 2.1\%\\
Label $=$ gold & 88.9\%\\
Label $=$ original author's label & 85.5\%\\
    \bottomrule
  \end{tabular}
    % From 1.0rc2
\caption{\label{b-table}Key statistics for the validated sentence pairs.} 
\end{table}

\begin{table}
\center
  %\setlength{\tabcolsep}{15pt}
  %\renewcommand{\arraystretch}{1.2}
  \begin{tabular}{l lll} 
    \toprule
\textbf{Label} & \textbf{Train} & \textbf{Dev} & \textbf{Test}\\
\midrule
\ii{entailment} &33.3 & 33.4 & 33.3 \\
\ii{neutral} & 33.2 & 33.2 & 33.2 \\
\ii{contradiction} & 33.3 & 33.2 & 33.3 \\
- (no consensus) & 0.2 & 0.2 & 0.2 \\
\bottomrule
  \end{tabular}
  % From 1.0rc2
\caption{\label{b-table}Frequency (\%) of each label in the corpus. Note that examples without gold labels are omitted in standard evaluations.} 
\end{table}


\paragraph{Partition} We distribute the corpus with a pre-specified train/test/development split. The test and development sets contain 10k examples each. Each original ImageFlickr caption occurs in only one of the three sets, and all of the examples in the test and development sets have gone through validation.


\paragraph{Parses}

The distributed corpus includes parses for every sentence produced by the Stanford PCFG Parser \cite{klein2003accurate}, both in the standard PTB format and in the binarized unlabeled format used by tree-structured neural network models, as in \S\ref{sentence-embedding}.

% TODO (CM?)

\paragraph{General evaluation standards}
While we hope that the corpus will be valuable in a variety of ways, we encourage researchers working on tools for semantic representation and inference to evaluate on our data in a uniform way: training on only the (parsed and/or unparsed) sentences included in the training set, and doing final evaluations on only the subset of the test set for which there are single gold labels.
