\begin{table*}[t]
  \centering\footnotesize
  \begin{tabular}{p{6.4cm}p{1.7cm}p{6.4cm}}
  \toprule
A man inspects the uniform of a figure in some East Asian country. & \fulllabel{contradiction}{c c c c c} & The man is sleeping\\
\rule{0pt}{3ex}An older and younger man smiling. & \fulllabel{neutral}{n n e n n} & Two men are smiling and laughing at the cats playing on the floor.\\
\rule{0pt}{3ex}A black race car starts up in front of a crowd of people. & \fulllabel{contradiction}{c c c c c} & A man is driving down a lonely road.\\
\rule{0pt}{3ex}A soccer game with multiple males playing. & \fulllabel{entailment}{e e e e e} & Some men are playing a sport.\\
\rule{0pt}{3ex}A smiling costumed woman is holding an umbrella. & \fulllabel{neutral}{n n e c n} & A happy woman in a fairy costume holds an umbrella.\\
    \bottomrule
% From 1.0rc3
  \end{tabular}
  \caption{\label{snli-examples}Randomly chosen examples from the development section of our new corpus, shown with both the gold labels and the full set of annotator labels (abbreviated).}
\end{table*}

\section{Introduction}\label{sec:introduction}

The semantic concepts of entailment and contradiction are central to
all aspects of natural language meaning
\cite{Katz72,vanBenthem08NATLOG}, from the lexicon to the content of
entire texts. Thus, \emph{natural language
  inference} (NLI) --- characterizing and using these relations in
computational systems
\cite{dagan2006pascal,MacCartney09,maccartney2009extended} --- is
essential in tasks ranging from information retrieval to semantic
parsing to commonsense reasoning.
\todo{Non-MacCartney NLI citation.}

NLI has been addressed using a variety of techniques, including
those based on sequence neural network models, knowledge bases, and symbolic
logic. In recent years, it has become an important testing ground for
approaches employing \emph{distributed} word and phrase
representations. Distributed representations excel at capturing
relations based in similarity, and they have proven effective at
modeling simple dimensions of meaning like evaluative sentiment
(e.g., \citealt{socher2013recursive}), but it is less clear that they can be
trained to support the full range of logical and commonsense
inferences required for NLI. In the SemEval 2014 task aimed at evaluating distributed
representations for NLI, the best-performing systems relied heavily on
additional features and reasoning capabilities
\cite{marelli2014semeval}. 

Our primary objective in this paper is to provide a new empirical
evaluation of a wide range of models for distributed semantic
representations in the context of NLI, thereby deepening the case for
NLI as a powerful evaluation of domain-general approaches to semantic
representation. However, in our view, existing NLI corpora do not
permit such an assessment. They are generally too small for training
modern data-intensive, wide-coverage models, many contain sentences
that were algorithmically generated, and they are often beset with
indeterminacies of event and entity coreference that significantly
impact annotation quality.

To address this, we present a new corpus of sentence pairs labeled for
entailment, contradiction, and semantic independence. At 570,152
sentence-pairs, this corpus is two orders of magnitude larger than all
other resources of its type. And, in contrast to many such resources,
all of the sentences were written by humans in a grounded,
naturalistic context. In a separate validation phase, we collected
four additional judgments for each label for 56,941 of the examples.
Of these, 98\% of cases emerge with a three annotator consensus, 
and 58\% see a unanimous consensus from all five annotators, 
which attests to the high quality of the data.

In this paper, we use this corpus to evaluate a wide variety of models
for natural language inference, including rule-based systems, simple
linear classifiers, and compositional neural networks. 
We find that two models achieve comparable performance: a feature-rich
classifier model and a neural network model which uses a Long Short-Term Memory network (LSTM; 
\citealt{hochreiter1997long}) to represent sentence meaning. We further evaluate the LSTM model
by taking advantage of its ready support for transfer learning, and show that it can be adapted to an existing
NLI challenge task, yielding the best reported performance for a neural network model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% One point of comparison that might be good to set up (maybe not in these words, this is Sam's sketch):

% Translation can also be used to train/evaluate NNs, and also demands some degree of sensitivity to compositional syntactic and semantic structure. Plus, it's easier to get good data for that task. But NLI is the better benchmark for developing NNs for language understanding, because:

% (i) Typical translation tasks require natural language generation, which is a separate difficult problem that must be learned in parallel with the semantic encoding task of interest, making results harder to interpret. We can just a vanilla well-understood classifier on top of our sentence model.

% (ii) Contradiction vs. entailment decisions in particular specifically target the abilities of NN models to learn lexical and phrasal representations (like alternation) that don't resemble similarity, either in their correlation with distributional information or their transitivity behavior. MT doesn't seem to have a good parallel to this. Since modeling similarity is almost the only aspect of NN behavior in NLP that's reasonably well understood and basically known to work, using a benchmark that explicitly demands something more sophisticated than this is likely to pay off by better exposing the weaknesses of current standard models.

% It might be also worth making an explicit comparison with sentiment as a benchmark, but that's low-hanging fruit.