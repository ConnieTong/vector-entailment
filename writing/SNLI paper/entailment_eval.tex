\section{SNLI as a platform for NLI research}

The most immediate application for the SNLI corpus is in developing models for the task of NLI. In particular, since SNLI is dramatically larger than any existing corpus of comparable quality, we expect it to make possible the use of parameter-rich models like neural networks for this task. In this section, we explore the performance of a range of standard and novel models trained on the corpus.

\subsection{Standard entailment models}
We evaluate a number of strong baselines on the SNLI corpus.
The first class of baselines use models from the Excitement Open
  Platform (EOP,
  \citealt{pado2014design,magnini2014excitement})
  -- an open source platform for RTE research.
%  which
%  is distributed alongside a number of RTE pipelines.
We additionally evaluate against a strong classifier-based baseline based on
  both  unlexicalized and lexicalized features.


%
% EOP
%
\paragraph{Excitement Open Platform}
% what is EOP
The Excitement Open Platform is a tool to quickly develop RTE systems
  while sharing components such as common lexical resources and 
  evaluation sets.
% 9 systems compared against
A number of systems have been built using the platform, 9 of them
  applicable to English are publicly distributed with version 1.2.1
  of the software.
% these fall into 2 classes
These fall into two classes of algorithms: 2 are edit distance based,
  whereas the remaining 7 make use of different features in a
  maximum entropy classifier.

%% methodology
%We convert the 3-way classification task in SNLI into the RTE setting
%  by labeling both the \unknown\ and \contradiction\
%  labels as negative entailment, and treating the \entailment\ label as
%  the positive entailment.
%This creates a biased dataset of 66\% negative examples.
% we report the best results from each class
We run the top performing edit-distance based algorithm and the top
  performing classifier-based algorithm on the SNLI test set, as
  determined by performance on the development set.
Note that these models were run using the default configuration
  with minimal tuning.
The results should therefore be taken as a strong baseline for
  RTE-style approaches to the problem, rather than necessarily
  representing the state-of-the-art system's performance on the
  task.
%We run each of the 9 algorithms distributed with EOP on the 2-class
%  SNLI dataset, and report results for the best edit distance 
%  configuration and the best classifier based configuration, as
%  determined by performance on the development set.

%
% EOP RESULTS TABLE
%
% Some definitions
\def\t#1{\small{#1}}
\def\b#1{\t{\textbf{#1}}}
\def\colspaceS{2.0mm}
\def\colspaceM{3.0mm}
\def\colspaceL{4.0mm}

% The table
\begin{table}
\begin{center}
\begin{tabular}{l@{\hskip \colspaceL}c@{\hskip \colspaceS}c@{\hskip \colspaceS}c@{\hskip \colspaceS}c@{\hskip \colspaceL}c@{\hskip \colspaceS}c}
\hline
\textbf{System} & \multicolumn{4}{c}{\b{SNLI}} & \multicolumn{2}{c}{\b{RTE-3}} \\
 & \t{P} & \t{R} & \t{F$_1$} & \t{Acc.} & \t{F$_1$} & \t{Acc.} \\
\hline
\t{Edit Distance} & \t{??.?} & \t{??.?} & \t{??.?} & \t{??.?} & 
                    \t{??.?} & \t{??.?} \\
\t{Classifier}    & \t{67.9} & \t{38.7} & \t{49.3} & \t{7X.X} & 
                    \t{64.2} & \t{65.3} \\
\hline
\end{tabular}
\end{center}
% The caption
\caption{
\label{tab:eopresults}
3-class accuracy and precision/recall of existing competitive RTE
systems from the Excitement Open Platform.
Results are reported for an edit-distance based algorithm, and a
  classifier-based system.
In order to calibrate the difficulty of the dataset against the 
  classical RTE problems, results are also reported on the (2-class) 
  RTE-3 dataset.
\todo{Final numbers}
}
\end{table}
%
% END EOP RESULTS TABLE
%

% the best systems
We report results in \reftab{eopresults}.
The best edit distance algorithm tunes the the weight of the three 
  case-insensitive edit distance operations on the training set, 
  after removing stop words.
The best classifier-based system makes use of information from
  WordNet \cite{miller1995wordnet} and VerbOcean
  \cite{chklovski2004verbocean}, and makes use of features
  based on tree patterns and dependency tree skeletons
  \cite{wang2007recognizing}.
Unsurprisingly, the classification-based approach outperforms simple
  edit distance metrics, and performs quite well despite relatively
  little lexicalization.

%
% Lexicalized Classifier
%
\paragraph{Lexicalized Classifier}
Unlike the RTE datasets, SNLI's size allows approaches which make use of
  rich lexicalized features.
We implmement such a lexicalized classifier as a strong baseline 
  for the task.
Our classifier implements 6 features:
\begin{enumerate}
\setlength\itemsep{-0.25em}
  \item the BLEU score of the \hypothesis\ with respect
  to the \premise, using an n-gram length between 1 and 4.

  \item The length difference between the \hypothesis\ and the \premise, as a real-valued
  feature.

  \item The overlap between words in the \premise\ and \hypothesis. We include
  both as an absolute count and a percentage of possible overlap, and compute
  the count both over all words and over just nouns, verbs, adjectives, 
  and adverbs.
  
  \item\label{lst:ngram} An indicator for every unigram and bigram in the \hypothesis.

  \item\label{lst:unigram} Cross-unigrams: for every pair of words across the \premise\ and \hypothesis\ which share a 
  POS tag, an indicator feature over the two words.
  
  \item\label{lst:bigram} Cross-bigrams: for every pair of bigrams across the \premise\ and \hypothesis\ which share a 
  POS tag, an indicator feature over the two bigrams.
\end{enumerate}

%
% BOW RESULTS TABLE
%

% The table
\begin{table}
\begin{center}
\begin{tabular}{l@{\hskip \colspaceL}c@{\hskip \colspaceL}c@{\hskip \colspaceS}c@{\hskip \colspaceS}c@{\hskip \colspaceM}c}
\hline
\textbf{System} & \b{Dev} & \multicolumn{4}{c}{\b{Test}} \\
 & \t{Acc.} & \t{P} & \t{R} & \t{F$_1$} & \t{Acc.} \\
\hline
\t{Lexicalized}            & \t{??.?} & \t{79.4} & \t{82.3} & \t{80.9} & \t{78.2} \\
\t{Unigrams Only}          & \t{??.?} & \t{71.1} & \t{77.7} & \t{74.3} & \t{71.6} \\
\t{Unlexicalized}          & \t{??.?} & \t{56.0} & \t{64.4} & \t{59.9} & \t{50.39} \\
\hline
\end{tabular}
\end{center}
% The caption
\caption{
\label{tab:bowresults}
Accuracy on the SNLI dataset, including precision/recall for the
  \textit{entailed} class on the test set.
Ablation results are reported for models lacking cross-bigram features 
  (Feature \ref{lst:bigram}), and lacking all lexical
  features (Features \ref{lst:ngram}--\ref{lst:bigram}).
\todo{Final numbers}
}
\end{table}
%
% END BOW RESULTS TABLE
%

% Commenting out until PDF available. -S
% Running 'make' should generate PDF from gnuplot
\Fig{learning_curves_bow.pdf}{0.45}{bowlearncurve}{
A learning curve for the lexicalized and unlexicalized baseline classifiers,
plotted on a log scale.
Note the y axis starting at a random-chance accuracy of 33\%.
}

% Results
We report results in \reftab{bowresults}, along with ablation studies for removing
  the bigram features (leaving only the unigram entailment pair feature),
  and for removing all lexicalized features.
In addition, we report learning curves for the lexicalized and fully unlexicalized
  classifiers in \reffig{bowlearncurve}.

% Insights 1: lexicalization helps a bunch
It is interesting to note the substantial jump in accuracy from using
  lexicalized features, and even from using the relatively sparse
  cross-bigram features.
In fact, the addition of these features alone allow the classifier to
  outperform the RTE baselines -- this is in some sense unsurprising:
  the RTE systems have been tuned on relatively small corpora
  ($\sim$1600 examples), whereas a classifier trained on SNLI can make use of
  over two orders of magnitude more data.

% Insights 2: the learning curve for the unlexicalized classifier is sad
More surprising is how quickly the lexicalized classifier outperforms its
  unlexicalized counterpart (see \reffig{bowlearncurve}).
With only 100 training examples, the cross-bigram classifier is already
  outperforming its unlexicalized counterpart.
Empirically we notice that the top weighted features for the classifier
  trained on 100 examples tend to be high precision entailments;
  e.g.,
  \textit{playing} $\rightarrow$ \textit{outside}
  (most scenes are outdoors), or \textit{a banana} $\rightarrow$
  \textit{person eating}.
If relatively few spurious entailments get high weight -- as it appears
  is the case -- then it makes sense that when these do fire they
  provide some boost in accuracy.


%\subsection{Existing NLI systems}

\subsection{Sentence embeddings and NLI}\label{sentence-embedding}

SNLI is uniquely suitable for training neural network models that produce vector representations of sentence meaning. In this section, we compare the performance of six such models on SNLI. 
In order to best evaluate the strengths of these six models at producing informative representations, we use sentence embedding as an intermediate step in the NLI classification task: each model most produce vector representation of each of the two sentences without using any context from the other sentence, and the two resulting vectors are then passed to a neural network classifier which predicts the label for the pair. This choice allows us to focus on evaluating the ability of existing models to produce representations that fully capture sentence meaning, at the possible cost of excluding from consideration neural models for NLI that directly compare the two inputs at the word or phrase level.

\input{model_structure_fig.tex}

Our neural network classifier, depicted in Fig.~\ref{modelstructure} is simply a stack of three 100d $\tanh$ layers, with the bottom layer taking the concatenated sentence representations as input and the top layer feeding a softmax classifier, all trained jointly with the representation model itself.

We test six sentence representation models, each set to use 50d word and phrase embeddings. Our baseline model in this experiment simply sums the embeddings of the word in each sentence. Next, we experiment with two recurrent models: a plain RNN, and a one-layer LSTM RNN \cite{hochreiter1997long}. Finally, we experiment with three tree-structured models: a plain TreeRNN \cite{socher2011semi}, a tensor-parameterized TreeRNTN \cite{socher2013acl1}, and a TreeLSTM \cite{tai2015improved}.

The word embeddings for all of the moedels are initialized the 300d reference GloVe vectors \cite{pennington2014glove} and fine tuned. In addition, all of the models use an additional $\tanh$ neural network layer to map these 300d embeddings into the lower-dimensianal phrase and sentence embedding space. All of the models are randomly initialized using standard techniques and trained using AdaDelta \cite{zeiler2012adadelta} minibatch SGD until performance on the development set stops improving. We applied L2 regularization to all models, manually tuning the strength coefficient $\lambda$ for each. All models were reimplemented in a common framework for this paper, and the implementations will be made available at publication time.

\begin{table}
\begin{center}
\begin{tabular}{l@{\hskip \colspaceL}@{\hskip \colspaceL}c@{\hskip \colspaceL}c}
\hline
\textbf{Sentence model} & \b{Train}  & \b{Test}\\
\hline
\t{100d Sum of words}            & \t{79?} & \t{76?} \\
\t{100d RNN}            & \t{67?} & \t{69?} \\	
\t{100d LSTM RNN}            & \t{75?} & \t{76?} \\
\t{100d TreeRNN}            & \t{69?} & \t{69?} \\
\t{50d TreeRNTN}            & \t{58?} & \t{56?} \\
\t{100d LSTM TreeRNN}            & \t{75?} & \t{73?} \\
\hline
\end{tabular}
\end{center}
% The caption
\caption{
\label{tab:nnresults}
Accuracy in 3-class classification on the SNLI training and test sets for each model.
\todo{Replace with final numbers/dims}
}
\end{table}

Note that \todo{something interesting happens}.
