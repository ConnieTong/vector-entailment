\subsection{Conclusion}\label{sec:conclusion}

Natural languages are powerful vehicles for complex, flexible
commonsense reasoning, and nearly all questions about meaningfulness
in language can be reduced to questions of contextual and entailment
and contradiction. This suggests that NLI is an ideal testing ground
for theories of semantic representation, and that training for NLI
tasks can provide rich domain-general semantic representations.  To
date, however, it has not been possible to fully realize this
potential due to the limited nature of existing NLI resources.  This
paper sought to remedy this with a new, large-scale, naturalistic
corpus of sentence pairs labeled for entailment, contradiction, and
independence. We used this corpus to evaluate a wide range of models,
and we found that not only do neural models perform best, but also
that their learned representations transfer well to new domains.
These results are powerful new evidence that neural representations
can be trained to accurately encode rich, multifaceted semantic
information.

