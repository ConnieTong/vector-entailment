\begin{abstract} 
Understanding entailment and contradiction is  fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. 
However, research in this area has been dramatically limited by the lack of large-scale resources. 
To address this, we introduce a new freely available corpus of labeled sentence pairs, written by humans in a novel grounded task framing based on image captioning. 
At 570K pairs, it is two orders of magnitude larger than all other resources of its type. 
We also use this corpus to evaluate several natural language inference models. We find that a simple Long Short-Term Memory (LSTM) based neural network model trained on our corpus not only performs best on our own evaluation set, but also achieves the best reported performance for a neural network model on an existing natural language inference benchmark.
\end{abstract}
