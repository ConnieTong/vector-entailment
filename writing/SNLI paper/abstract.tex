\begin{abstract} 
  % Entailment and contradiction are fundamental to natural language
  % meaning and lie at the heart of tasks like summarization, textual
  % entailment, and commonsense reasoning.
  Entailment and contradiction are fundamental to natural language
  meaning and so have emerged (in the form of tasks involving
  reasoning, inference, and summarization) as an important testing
  ground for semantic representations.
  %
  %However, computational modeling of these concepts is currently
  %hampered by the limited nature of existing annotated resources.
  However, progress is currently hampered by the limitations of
  existing annotated resources.
  %
  To address this, we present a new corpus of sentence pairs labeled
  for entailment, contradiction, and semantic independence. At 570K
  pairs, it is orders of magnitude larger than all other resources
  of its type.
  %
  And, in contrast to many such resources, all of the sentences were
  written by humans in a grounded, naturalistic context.
  %
  In this paper, we use this corpus to evaluate a wide variety of
  models for natural language inference, finding that Tree-Structured
  Long Short-Term Memory networks (TreeLSTMs) achieve the best
  performance.
  %
  In addition, to enhance the case for TreeLSTMs, and deepen the
  argument for the centrality of entailment and contradiction in
  semantics, we show that the TreeLSTM's representations, trained on
  our corpus, perform well in a disparate set of additional semantic
  tasks.  
  \todo{Structure more around results (vs. data).}
\end{abstract}
