\begin{abstract} 
Understanding entailment and contradiction is  fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. 
However, research in this area has been dramatically limited by the lack of large-scale resources. 
To address this, we introduce a new freely available corpus of labeled sentence pairs, written by humans in a novel grounded task framing based on image captioning. 
At 570K pairs, it is two orders of magnitude larger than all other resources of its type. 
In this paper, we use this corpus to evaluate a wide variety of natural language inference models, finding that Tree-Structured Long Short-Term Memory networks (TreeLSTMs) achieve the best performance. 
In addition, we show that models trained on our corpus perform competitively on an existing NLI test set.
\end{abstract}
