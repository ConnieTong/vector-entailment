\begin{abstract} 
Understanding entailment and contradiction is  fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce a new freely available corpus of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. We find that this increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and that it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.
\end{abstract}
