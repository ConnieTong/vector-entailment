\begin{abstract} 
In a tree-structured recursive neural network model, the meaning of a sentence is encoded using a neural network that is structured in a way that mirrors the syntactic structure of the sentence produced by a parser. This kind of syntactic structure is potentially valuable, but the reliance on an external parser is not ideal. There is no guarantee that the parsing formalisms used by freestanding syntact parsers are at all suitable for guiding semantic composition. We present a neural network model that is able to used a joint syntactic and semantic objective to learn to simultaneously discover semantically appropriate syntactic structures and to use those structures to produce semantic representations for some downstream task. We show that this model can reach state of the art performance on sentiment classification, and that it shows hints of being able to learn both syntactic and semantic representations that could be valuable in more demanding language understanding tasks.
\end{abstract}
