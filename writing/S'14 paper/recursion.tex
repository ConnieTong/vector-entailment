\section{Recursive structure in propositional logic}\label{sec:recursion}

Recursive structure is a prominent feature of natural languages and an
important part of their expressivity, as it allows people to use and
interpret complex structures they have never encountered before.
Theoretical accounts of natural language syntax and semantics
therefore rely heavily on recursive structures, and we expect that
accurate computational models will have to come to grips with them as
well. The present section pursues this question for our two classes of
RNN. In our evaluations, we exploit the fact that our logical language
is infinite by testing on strings that are longer and more complex
than any seen in training.

% Consider, for example, \ii{Alice said hello}, \ii{Bob said that Alice
%   said hello}, and \ii{Carl thinks that Bob said that Alice said
%   hello}. Overt recursion of this kind is easy to find, and
% theoretical accounts of natural language syntax and semantics rely
% heavily on recursive structures. In order for a model to be able to
% accurately learn natural language meanings, then, we expect that it
% would need to be able to learn to represent the meanings of function
% words in a such a way that they are able to behave correctly when
% taking their own outputs as input. In evaluating the model, we take
% advantage of the fact that recursive structures of this kind define
% potentially infinite languages by testing the model on strings that
% are longer and more complex than any seen in testing.

\paragraph{Experiments}
As in Section~\ref{sec:join}, we test this phenomenon within the
framework of natural logic, but we now replace the unanalyzed symbols
from that experiment with complex formulae. These formulae
represent a truth-functionally complete classical propositional logic:
each atomic variable denotes either $\True$ or $\False$, and the only
operators are truth-functional ones.  Table~\ref{tab:pl} defines this
logic, and Table~\ref{tab:plexs} gives some examples of relational
statements that we can make in these terms. To compute these relations
between statements, we exhaustively enumerate the sets of assignments
of truth values to propositional variables that would satisfy each of
the statements, and then we convert the set-theoretic relation between
those assignments into one of the seven relations in
Table~\ref{b-table}. As a result, each relational statement represents
a valid theorem of the propositional logic.

\begin{table}[tp]
  \centering
  \begin{subtable}[t]{0.45\textwidth}
    \centering
    \begin{tabular}[t]{l l}
      \toprule
      Formula     & Interpretation \\
      \midrule
      $a$, $b$, $c$, $d$, $e$, $f$ & $\sem{x} \in \{\True, \False\}$ \\
      $\plneg \varphi$ & $\True$ iff $\sem{\varphi} = \False$ \\
      $(\varphi \pland \psi)$ & $\True$ iff $\False \notin \{\sem{\varphi}, \sem{\psi}\}$ \\
      $(\varphi \plor \psi)$  & $\True$ iff $\True \in \{\sem{\varphi}, \sem{\psi}\}$ \\
      \bottomrule
    \end{tabular}    
    \caption{Well-formed formulae. $\varphi$ and $\psi$
      range over all well-formed formulae, and $\sem{\cdot}$ is
      the interpretation function mapping formulae into $\{\True,
      \False\}$.}\label{tab:pl}
  \end{subtable}
  \quad
  \begin{subtable}[t]{0.45\textwidth}
    \centering
    \begin{tabular}[t]{r c l}
      \toprule
      $\plneg a$        & $\natneg$ & $a$ \\
      $\plneg \plneg a$ & $\nateq$  & $a$ \\
      $a$               & $\natfor$ & $(a \plor b)$ \\
      $a$               & $\natrev$ & $(a \pland b)$ \\
      %$(a \natfor b)$   & $\nateq$  & $(b \natrev a)$ \\	
      $\plneg(\plneg a \pland \plneg b)$ & $\nateq$ & $(a \plor b)$ \\ 
      \bottomrule
    \end{tabular}
    \caption{Examples of statements about relations between
      well-formed formulae, defined in terms of sets of satisfying
      interpretation functions $\sem{\cdot}$.}\label{tab:plexs}
  \end{subtable}
  \caption{Propositional logic with natural logic relations.}  
  \label{prop-figure}
\end{table}

% NOTE: There was some confusion at CSLI and the 224U lecture about
% whether this was theorem proving or some kind of model checking.
% Hopefully this is explicit enough.
Socher et al.~\cite{socher2012semantic} show that a matrix-vector RNN
model somewhat similar to our RNTN can learn a logic, but the logic
discussed here is a much better approximation of the kind of
expressivity needed for natural language. The logic learned in that
experiment is boolean, wherein the atomic symbols are simply the
values $0$ and $1$, rather than variables over those values. While
learning the operators of that logic is not trivial, the outputs of
each operator can be represented accurately by a single bit. The
statements of propositional logic learned here describe conditions on
unknown truth values of propositions. As opposed to the two-way
contrasts seen in \cite{socher2012semantic}, this logic distinguishes
between $2^{6} = 64$ possible assignments of truth values, and
expressions of this logic define arbitrary conditions on these
possible assignments, for a total of $2^{64}$ %($\approx 10^{20}$)
possible statements that the intermediate vector representations need
to be able to distinguish if we are to learn this logical system in its
full generality.

For our experiments, we randomly generate a large set of  unique pairs 
of formulae and compute the relation that holds for each pair.
We discard pairs in which either statement is a tautology or
contradiction (a statement that is true of either all or no possible
assignments), for which none of the seven relation labels in
Table~\ref{b-table} can hold. The resulting set of formula pairs is
then partitioned into bins based on the number of logical operators in
the longer of the two formulae. We then randomly sample 15\% of each
bin for a held-out test set.

If we do not implement any constraint that the two statements being
compared are similar in any way, then the generated data are dominated
by statements in which the two formulae refer to largely separate
subsets of the six variables, which means that the $\natind$ relation
is almost always correct.  In an effort to balance the distribution of
relation labels without departing from the basic task of modeling
propositional logic, we disallow individual pairs of statements from
referring to more than four of the six propositional variables.

\paragraph{Results} 
We trained both the RNN and RNTN models on data with up to 4
connectives (65k pairs) and tested it on examples of up to 12
connectives (44k pairs). We initialized the model parameters randomly,
including the vector representations of the six variables. The results
are shown in Figure~\ref{prop-results}. In tuning, we found that the
RNN model was approximately optimal with 45-dimensional vector
representations, and the RNTN model was approximately optimal with 25
dimensions. We fixed the size of the feature vector for the classifier
at 75 dimensions. We found that the RNTN model was able to perform
almost perfectly on unseen small test examples, with accuracy above
98.7\% on formulae below length four (macro-F1 was above 95.5).
Starting at size four, performance gradually falls with increasing
size.  The RNN model did not perform well, reaching only 90.5\% (81.4
micro-F1) accuracy on the smallest test examples, and declining from
there to near-baseline performance on formulae of length 12. Training
accuracy was 99.6\% for the RNTN and 87.6\% for the RNN.

% thu-and-or-deep-25-32b.txt iter 136
% mon-and-or-deep-45r-32.txt iter 480

\begin{figure}[tp]
  \centering
  \includegraphics[width=5.3in]{recursion\string_results\string_final.eps}
  \caption{Model performance on propositional logic, by expression size. 
    The dashed line indicates that only expressions of size four or less appeared in the training data.}  
  \label{prop-results}
\end{figure}

The performance of the RNTN model on small unseen test examples
indicates that it learned a correct approximation of the underlying
logic. It appears that this approximation is accurate enough to yield
correct answers when the composition layer is only applied a small
number of times, but that the error in the approximation grows with
increasing depth (and with the increasing complexity of the
expressions), resulting in gradually dropping performance. In
contrast, the RNN model did not appear to be able to learn a correct
approximation of the logic for statements of any length, but it
appears that the faulty logic that the RNN did learn decayed similarly
with expression size.  This is not necessarily a significant flaw in the model, since it remains possible that  different training regimes would allow it to learn accurate representations of the larger expressions. 


In an effort to better understand why the RNTN outperformed the plain
RNN, we evaluated both models on pairs of long formulae involving
binary connectives, to assess the extent to which they achieve
substantially different representations for semantically distinct
formulae and substantially similar representations for synonymous
formulae. We found that both models are able to identify synonymous
pairs like $(a \pland a)$ and $(a \pland (a \pland (a \pland a)))$,
with both \ii{and} and \ii{or}, but only the RNTN is able to learn
substantially different representations for pairs of differing formulae like $(a \pland (a \pland a))$
 and $(a \pland (a \pland b))$ when the difference between the two is embedded under multiple operators. 
 Figure~\ref{prop-falloff} summarizes this result.


% we discovered that this model looses information about in longer
% statements in a particularly problematic way. It appears that that
% model is unable to distinguish between two sentences when the only
% difference between those sentences is embedded within a very deep
% structure. We evaluated both models on sentences that differed in only
% one term, but for which that one term was embedded under a large
% number of conjunctions, such as the pair \ii{a (and a)} and \ii{a (and
%   b)}, or the pair \ii{a (and (a (and a)))} and \ii{a (and (a (and
%   b)))}. We then measured the Euclidean distance between the vector
% representations of the two sentences in each pair. Our findings are
% shown in Figure~\ref{prop-falloff}, and show that while the RNTN can
% distinguish the two sentences well at every size that we test, the RNN
% fails after a depth of approximately six.

\begin{figure}[htp]
  \centering
  \includegraphics[width=5.3in]{recursion\string_pairwise.eps}
  \caption{Semantically distinct formulae should have different
    representations. As measured by Euclidean distance, only the RNTN
    achieves this for formulae containing more than a small number of
    connectives (\ii{and} in this example). The RNN quickly collapses the representations,
    failing to capture the meaning contrasts.}
  \label{prop-falloff}
\end{figure}


