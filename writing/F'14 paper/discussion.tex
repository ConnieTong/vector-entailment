\section{Discussion and conclusion}\label{sec:discussion}

This paper first evaluates two recursive models on a series of three increasingly
challenging interpretive tasks involving natural language inference---the 
core relational algebra of natural logic with entailment and
exclusion; recursive propositional logic structures; and statements
involving quantification and negation. We then show that the same models can learn to
perform an entailment task on natural language image captions. The results suggest that RNTNs,
and potentially also RNNs, have the capacity to learn to model these tasks with 
reasonably-sized training sets. These positive results are
promising for the future of learned representation models in the
applied modeling of compositional semantics.

% TODO: Maybe trim after this point.
% Shorter promisory note, then summary of contributions.

Of course, challenges remain. Even
the RNTN falls short of perfection in the recursion experiment, with
performance falling off steadily as the size of the expressions grows. It
remains to be seen whether these deficiencies can be overcome with
stronger models or optimization procedures. The space of possible stronger
models is sizable, including extensions of the recursive models used here
\cite{sochergrounded,kalchbrenner2014convolutional,irsoydeep}, recurrent
models \cite{sutskever2014sequence}, and even learning-enabled variants 
of more structured models like those of \newcite{grefenstette2013towards} or \newcite{rocktaschellow}.
In addition, interesting analytical questions remain about \ii{how} these models encode
the underlying logics. Neither the underlying
logical theories, nor any straightforward parameter inspection technique provides 
much insight on this point, but we hope that further experiments may reveal 
some structure in the representations.

Our artificial data experiments have only scratched the surface 
of the logical complexity of natural language; in future experiments, we hope to test sentences
with more complex types of structure, as well as run similar experiments on non-tree structured models, such as LSTMs. Our SICK experiments 
similarly only begin to reveal the potential of these models to learn to 
perform complex semantic inferences from corpora. Nonetheless, the
rapid progress the field has made with these models in recent years
provides ample reason to be optimistic that they can be trained to
meet all the challenges of natural language semantics.

% These experiments represent one of the first attempts to reproduce any large fragment of the behavior of a complex logic within a neural network model, and the first attempt that we are aware of to address either the encoding of lexical relations or the learning of recursive operators. This presents considerable challenges in evaluating the particular models that we choose, since we cannot rely on prior results to establish that any particular amount or type of training data is sufficient to teach any model the structure of the logic. The positive results that we have found, however, are extremely promising for the future of learned representation models in the applied modeling of meaning. We have seen that recursive neural tensor networks are able to encode lexical relations accurately and encode recursive operators. We have also seen that both RNNs and RNTNs are able to handle the meanings of quantifiers in an inference setting in at least some cases. 

% There is ample room to build on these results. In the interest of fully mirroring the capacity of existing natural logics in learned models, it would be valuable to extend these experiments to cover other ways in which meanings are encoded in natural language, including challenges such as reasoning over sentences with transitive verbs or relative clauses. In addition, it would be highly informative to compare these results on standard recursive neural networks with other proposed learned models for sentence meaning, such as dependency tree RNNs \cite{sochergrounded}, Belief Propagation RNNs (-), or convolutional RNNs \cite{kalchbrenner2014convolutional}.

