\section{Discussion and conclusion}\label{sec:discussion}

This paper first evaluates two recursive models on a series of three increasingly
challenging interpretive tasks involving natural language inference---the 
core relational algebra of natural logic with entailment and
exclusion; recursive propositional logic structures; and statements
involving quantification and negation. We then show that the same models can learn to
perform an entailment task on natural language image captions. The results suggest that TreeRNTNs,
and potentially also TreeRNNs, have the capacity to learn to model these tasks with 
reasonably-sized training sets. These positive results are
promising for the future of learned representation models in the
applied modeling of compositional semantics.

Some questions about the abilities of these models remain open. Even
the TreeRNTN falls short of perfection in the recursion experiment, with
performance falling off steadily as the size of the expressions grows. It
remains to be seen whether these deficiencies can be overcome with
stronger models or learning techniques. In addition, interesting 
analytical questions remain about \ii{how} these models encode
the underlying logics. Neither the underlying
logical theories, nor any straightforward parameter inspection technique provides 
much insight on this point, but we hope that further experiments may reveal 
some structure in the representations.

Our SICK experiments similarly only begin to reveal the potential of these models to learn to 
perform complex semantic inferences from corpora, and there is ample room to develop our understanding
using new sources of natural language data. Nonetheless, the rapid progress the field 
has made with these models in recent years provides ample reason to be optimistic that 
learned representation models can be trained to
meet all the challenges of natural language semantics.

