\section{The SICK textual entailment challenge}\label{sec:sick}



The specific model architecture that we use is novel, and though the underlying tree structure approach has been validated elsewhere, our experiments so far do not guarantee that it viable model for handling inference over real
natural language data. To investigate our models' ability to handle the noisy labels and the diverse range of linguistic structures seen in typical natural language data, we use the SICK textual entailment challenge corpus \cite{marelli2014sick}. The corpus consists of about 10k natural language sentence pairs, labeled with \ii{entailment}, \ii{contradiction}, or \ii{neutral}. At only a few thousand distinct sentences (many of them variants on an even smaller set of template sentences), the corpus is not large enough to train a high quality learned model of general natural language, but it is the largest human-labeled entailment corpus that we are aware of, and our results nonetheless show that tree-structured NN models can learn to do inference in the real world.

Adapting to this task requires us to make a few additions to the techniques discussed in \S\ref{methods}. In order to better handle rare words, we initialized our word embeddings using 200 dimensional vectors trained with 
GloVe \cite{pennington2014glove} on data from Wikipedia. Since 200 dimensional vectors are too large to be practical in an TreeRNTN on a small dataset, a new embedding transformation layer is needed. Before any embedding is used as an input to a recursive layer, it is passed through an additional $\tanh$ neural network layer with the same output dimension as the recursive layer. This new layer aggregates any usable information from the embedding vectors into a more compact working representation. An identical layer is added to the SumNN between the word vectors and the comparison layer.

We also supplemented the SICK training data\footnote{We tuned the model using performance on a held out development set, but report performance here for a version of the model trained on both the training and development data and tested on the 4,928 example SICK test set. We also report training accuracy on a small sample from each data source.} with 600k examples of entailment data from the Denotation Graph project (DG, \citealt{hodoshimage}, also used by the winning SICK submission), a corpus of noisy automatically labeled entailment examples over image captions, the same genre of text from which SICK was drawn. We trained a single model on data from both sources, but used a separate set of softmax parameters for classifying into the labels from each source. We parsed the data from both sources with the Stanford PCFG Parser v.~3.3.1 \cite{klein2003accurate}. We also found that we were able to train a working model much more quickly with an additional technique: we collapse subtrees that were identical across both sentences in a pair by replacing them with a single head word. The training and test data on which we report performance are collapsed in this way, and both collapsed and uncollapsed copies of the training data are used in training. Finally, in order to improve regularization on the noisier data, we used dropout \cite{srivastava2014dropout} at the input to the comparison layer (10\%) and at the output from the embedding transform layer (25\%). 

\begin{table}[tp]
  \centering \small
    \begin{tabular}{ l@{\hspace{-0.25em}} r@{~~~~} r@{~~~~} r@{~~~~} r@{~~~~} }
    \toprule
        ~&\ii{neutral}&	 30d  & 			30d & 50d\\
    ~&only &SumNN  &TrRNN &TrRNTN\\ 
     \midrule
    DG Train	& 50.0 & 68.0 & 67.0 & \textbf{74.0} \\
    SICK Train	& 56.7 & 96.6 & 95.4 & \textbf{97.8} \\
    SICK Test	& 56.7 & 73.4 & 74.9 & \textbf{76.9} \\
    \midrule
    \textsc{Passive} (4\%)	& 0 		& 76  		& 68		&\textbf{88}\\   
    \textsc{Neg} (7\%)		& 0 		& 96	 		& \textbf{100} & \textbf{100}\\
    \textsc{Subst} (24\%)	& 28 		& \textbf{72}  		& 64 		&  \textbf{72}\\
    \textsc{MultiEd} (39\%)	&  \textbf{68} & 61  		&66 		& 64 \\
    \textsc{Diff} (26\%)		& \textbf{96} &  	68		&79		& \textbf{96}\\  
    \midrule
    \textsc{Short} (47\%) & 50.0 & 73.9 & 73.5		& \textbf{77.3} \\    
    \bottomrule
  \end{tabular}
  \caption{Classification accuracy, including a category breakdown for SICK test data. Categories are shown with their frequencies.}
  \label{sresultstable}
\end{table} 

\begin{table*}[htp]
  \centering\small
  \begin{tabular}{l@{~~~}cl}
    \toprule
  The patient is being helped by the doctor	& \ii{entailment} & The doctor is helping the patient (\textsc{Passive})\\
    A little girl is playing the violin on a beach & \ii{contradiction} &	There is no girl playing the violin on a beach (\textsc{Neg})\\
    
    The yellow dog is drinking water from a bottle& \ii{contradiction} &	The yellow dog is drinking water from a pot  (\textsc{Subst})\\
        A woman is breaking two eggs in a bowl & \ii{neutral} &A man is mixing a few ingredients in a bowl (\textsc{MultiEd})\\
        Dough is being spread by a man & \ii{neutral} & A woman is slicing meat with a knife (\textsc{Diff})\\
    \bottomrule
  \end{tabular}
  \caption{\label{examplesofsickdata}Examples of each category used in error analysis from the SICK test data. }
\end{table*}


\paragraph{Results} Despite the small amount of high quality training data available and the lack of resources for learning lexical relationships, the results (Table~\ref{sresultstable}) show that our tree-structured models perform competitively on textual entailment, beating a strong baseline. Neither model reached the performance of the winning system (84.6\%), but the TreeRNTN did exceed that of eight out of 18 submitted systems, including several which used sophisticated hand-engineered features and lexical resources specific to the version of the entailment task at hand. 

To better understand our results, we manually annotated a fraction of the SICK test set, using mutually exclusive categories for passive/active alternation pairs (\textsc{Passive}), pairs differing only by the presence of negation (\textsc{Neg}), pairs differing by a single word or phrase substitution (\textsc{Subst}), pairs differing by multiple edits (\textsc{MultiEd}), and pairs with little or no content word overlap (\textsc{Diff}). Examples of each are in Table \ref{examplesofsickdata}. We annotated 100 random examples to judge the frequency of each category, and  continued selectively annotating until each category contained at least 25. We also use the category \textsc{Short} for pairs in which neither sentence contains more than ten words.
 
The results (Table \ref{examplesofsickdata}) show that the TreeRNTN performs especially strongly in the two categories which pick out specific syntactic configurations, \textsc{Passive} and \textsc{Neg}, suggesting that that model has learned to encode the relevant structures well. It also performs fairly on \textsc{Subst}, which most closely parallels the lexical entailment inferences addressed in \S\ref{sec:quantifiers}. In addition, none of the models perform dramatically better on the \textsc{Short} pairs than on the rest of the data, suggesting that the performance decay observed in \S\ref{sec:recursion} may not impact models trained on typical natural language text.

It is known that a model can perform well on SICK (like other natural language inference corpora) without taking advantage of compositional syntactic or semantic structure \cite{marelli2014semeval}, and our summing baseline model is powerful enough to do this. Our tree models nonetheless perform substantially better, and we remain confident that given sufficient data, it should be possible for the tree models, and not the summing model, to learn a truly high-quality solution.
