\section{Introduction}\label{sec:intro}

Tree-structured recursive neural network models (TreeRNNs; \citealt{goller1996learning}) for sentence meaning
have been successful in an array of sophisticated language tasks,
including sentiment analysis \cite{socher2011semi,irsoydeep},
image description \cite{sochergrounded}, and paraphrase detection
\cite{Socher-etal:2011:Paraphrase}. These results are encouraging for
the ability of these models to learn to produce and use strong 
semantic representations for sentences. However, it remains an 
open question whether any such fully learned model can achieve the kind of
high-fidelity distributed representations proposed in recent algebraic
work on vector space modeling 
\cite{ClarkCoeckeSadrzadeh2011,grefenstette2013towards,Hermann-etal:2013,rocktaschellow},
and whether any such model can match the performance of grammars based in logical forms
in their ability to model core semantic phenomena like quantification, entailment, and contradiction \cite{Warren:Pereira:1982,Zelle:Mooney:1996,ZetCol:2005,LiangJordan:2013}.

Recent work on the algebraic approach of
\newcite{ClarkCoeckeSadrzadeh2011} has yielded rich frameworks 
for computing the meanings of fragments of natural
language compositionally from vector or tensor representations, but 
has not yet yielded effective methods for learning these representations
from data in typical machine learning settings. 
Past experimental work on reasoning with
distributed representations have been largely confined to short
phrases \cite{Mitchell:Lapata:2010,Grefenstette-etal:2011,baroni2012entailment}.
However, for robust natural language understanding, it is essential to
model these phenomena in their full generality on complex linguistic
structures. 

This paper describes four machine learning experiments
that directly evaluate the abilities of these models to learn representations that 
support specific semantic behaviors. These tasks follow the format of
 \ii{natural language inference} 
(also known as \ii{recognizing textual entailment};
\citealt{dagan2006pascal}), in which the goal is to determine the core
inferential relationship between two sentences. 
We introduce a novel NN architecture for natural language inference which
independently computes vector representations for each of two sentences using
standard TreeRNN or TreeRNTN \cite{socher2013acl1} models, and
produces a judgment for the pair using only those representations. This allows
us to gauge the abilities of these two models to represent all of the necessary semantic
information in the sentence vectors. 

Much of the
theoretical work on natural language inference (and some successful implemented models;
\citealt{maccartney2009extended,watanabe2012latent}) involves \ii{natural
  logics}, which are formal systems that define rules of inference
between natural language words, phrases, and sentences without the
need of intermediate representations in an artificial logical
language.
In our first three experiments, we test our models' ability to learn the foundations of natural
language inference by training them to reproduce the behavior of the natural logic of \newcite{maccartney2009extended} 
on artificial data. This logic
defines seven mutually-exclusive relations of synonymy, entailment, contradiction,
and mutual consistency, as summarized in Table~\ref{b-table}, and it
provides rules of semantic combination for projecting these relations
from the lexicon up to complex phrases. The formal properties of this system 
are now well-understood \cite{Icard:Moss:2013,Icard:Moss:2013:LILT}.
The first experiment using this logic 
covers reasoning with the bare logical relations (\S\ref{sec:join}), the second extends this to reasoning with statements constructed compositionally from recursive functions (\S\ref{sec:recursion}),
and the third covers the additional complexity that results from quantification (\S\ref{sec:quantifiers}).
Though the performance of the plain TreeRNN model 
 is somewhat poor in our first experiment, we find that the stronger TreeRNTN model
  generalizes well in every case, suggesting that it has learned to simulate our target logical concepts.

The experiments with simulated data provide a convincing demonstration
of the ability of neural networks to learn to build and use semantic representations for complex natural language sentences from reasonably-sized
training sets. However, we are also interested in the more practical
question of whether they can learn these representations from
naturalistic text. To address this question, we apply our models to
the SICK entailment challenge data in \S\ref{sec:sick}. The small size of this corpus puts
data-hungry NN models like ours at a disadvantage, but we are
nonetheless able to achieve competitive performance on it,
surpassing several submitted models with significant hand-engineered task-specific features and our own NN baseline.
This suggests that the representational abilities that we observe in the previous sections
are not limited to carefully circumscribed tasks. We conclude that TreeRNTN models
are adequate for typical cases of natural language inference, and that there is not yet
any clear level of inferential complexity for which other approaches work and NN models fail.

\begin{table*}[tp]
  \centering\small
  \setlength{\tabcolsep}{15pt}
  \renewcommand{\arraystretch}{1}
  \begin{tabular}{l c l l} 
    \toprule
    Name & Symbol & Set-theoretic definition & Example \\ 
    \midrule
    entailment         & $x \natfor y$   & $x \subset y$ & \ii{turtle, reptile}  \\ 
    reverse entailment & $x \natrev y$   & $x \supset y$ & \ii{reptile, turtle}  \\ 
    equivalence        & $x \nateq y$    & $x = y$       & \ii{couch, sofa} \\ 
    alternation        & $x \natalt y$   & $x \cap y = \emptyset \wedge x \cup y \neq \mathcal{D}$ & \ii{turtle, warthog} \\ 
    negation           & $x \natneg y$   & $x \cap y = \emptyset \wedge x \cup y = \mathcal{D}$    & \ii{able, unable} \\
    cover              & $x \natcov y$   & $x \cap y \neq \emptyset \wedge x \cup y = \mathcal{D}$ & \ii{animal, non-turtle} \\ 
    independence       & $x \natind y$   & (else) & \ii{turtle, pet}\\
    \bottomrule
  \end{tabular}
  \caption{\label{b-table}The seven relations of MacCartney and Manning \protect\shortcite{maccartney2009extended}'s logic are defined abstractly on pairs of sets drawing from the universe $\mathcal{D}$, but can be straightforwardly applied to any pair of natural language words, phrases, or sentences.} %-%
 
\end{table*}



























































