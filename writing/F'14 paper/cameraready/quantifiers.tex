\section{Reasoning with quantifiers and negation}\label{sec:quantifiers}

We have seen that recursive models can learn an approximation of propositional
logic.  However, natural languages can express functional meanings of
considerably greater complexity than this.  As a key test of whether our models 
can capture this complexity, we now study the degree to which they are able to
develop suitable representations for the semantics of natural language
quantifiers like \ii{most} and \ii{all} as they interact with negation and lexical entailments. Quantification 
and negation are far from the only place in natural language where complex functional meanings
are found, but they are natural focus, since they have
formed a standard case study in prior formal work on natural
language inference \cite{Icard:Moss:2013:LILT}.

\paragraph{Experiments}
Our data consist of pairs of sentences generated
from a grammar for a simple English-like artificial language.
Each sentence contains a quantifier, a noun
which may be negated, and an intransitive verb which may be
negated. We use the quantifiers \ii{some}, \ii{most}, \ii{all},
\ii{two}, and \ii{three}, and their negations \ii{no}, \ii{not-all},
\ii{not-most}, \ii{less-than-two}, and \ii{less-than-three}, and also
include five nouns, four intransitive verbs, and the negation symbol
\ii{not}. In order to be able to define relations between sentences
with differing lexical items, we define the lexical relations for
each noun--noun pair, each verb--verb pair, and each
quantifier--quantifier pair. The grammar then generates pairs of
sentences and calculates the relations
between them.  
For instance, our models might then see
pairs like \eqref{p1} and \eqref{p2} in training and be required to 
then label \eqref{p3}.

\vspace{-0.6cm}
\begin{gather}
  \text{(most turtle) swim} \natalt \text{(no turtle) move}\label{p1}
  \\
  \text{(all lizard) reptile} \natfor  \text{(some lizard) animal}\label{p2}
  \\
  \text{(most turtle) reptile} \natalt \text{(all turtle) (not animal)}\label{p3}
\end{gather}

%nouns = ['warthogs', 'turtles', 'mammals', 'reptiles', 'pets']
%verbs = ['walk', 'move', 'swim', 'growl']
%dets = ['all', 'not_all', 'some', 'no', 'most', 'not_most', 'two', 'lt_two', 'three', 'lt_three']
%adverbs = ['', 'not']

% To assign relation labels to sentence pairs, we built a small
% task-specific implemenation of MacCartney's logic that can
% accurately label sentences of this restricted language. The logic is
% not able to derive all intuitively true relations of this language,
% and fails to derive a single unique relation for certain types of
% statement, including De Morgean's laws (e.g. \ii{(all pets) growl
% $\natneg$ (some pet) (not growl)}), and we simply discard these
% examples. Exhaustively generating the valid sentences under this
% grammar and choosing those to which a relation label can be assigned
% yields 66k sentence pairs. Some examples of these data are provided
% in Table~\ref{examplesofdata}.

In each run, we randomly partition the set of valid \textit{single sentences} into train and test, and then label all of the pairs from within each set to generate a training set of 27k pairs and a test set of 7k pairs. Because the model doesn't see the test sentences at training time, it cannot directly use the kind of reasoning described in \S\ref{sec:join} at the sentence level (by treating sentences as unanalyzed symbols), and must instead jointly learn the word-level relations and a complete reasoning system over them for our logic. 

We use the same summing baseline as in \S\ref{sec:recursion}.
The highly consistent  sentence structure in this experiment means that this model
is not as disadvantaged by the lack of word order information as it is in the previous experiment, 
but the variable placement of \ii{not}  nonetheless introduces potential uncertainty in the 58.8\% 
of examples that contain a sentence with a single token of it.

\begin{table}[tp]
  \centering\small
    \begin{tabular}{ l r@{ \ }r r@{ \ }r }
    
    \toprule
    ~ & \multicolumn{2}{c}{Train} & \multicolumn{2}{c}{Test} \\
    \midrule
    $\natind$ only &	35.4 & (7.5)	& 35.4	& (7.5)\\
    25d SumNN	&	96.9&	(97.7)&	93.9&	(95.0)\\	
    25d TreeRNN	&	99.6&	(99.6)&	99.2&	(99.3)\\
    25d TreeRNTN	&	\textbf{100}&(\textbf{100})&	\textbf{99.7} & \textbf{(99.5)}\\
    \bottomrule
  \end{tabular}
  
  \caption{Performance on the quantifier experiments, given as \% correct and macroaveraged F1.}
  \label{qresultstable}
\end{table} 

%
% do not allow a blank line --- adds too much space
%
\paragraph{Results} The results (Table~\ref{qresultstable}) show that both tree models are able to learn to generalize the underlying logic almost perfectly. The baseline summing model can largely memorize the training data, but does not generalize as well. We do not find any consistent pattern in the handful of errors made by either tree model, and no errors were consistent across model restarts, suggesting that there is no fundamental obstacle to learning a perfect model for this problem.

