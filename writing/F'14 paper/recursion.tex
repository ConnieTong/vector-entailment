\section{Recursive structure}\label{sec:recursion}

Complex embedding is a prominent feature of natural languages. 
The principle of compositionality explains how the meanings of 
such structures are derived systematically from their parts,
 enabling people to use novel phrases and sentences.
Theoretical accounts of natural language syntax and semantics
in turn rely heavily on recursive structures. This section examines the 
question of whether our models are able to learn a 
compositional semantics over such recursive structures.
In our evaluations, we exploit the fact that our logical language
is infinite by testing on strings that are longer and more complex
than any seen in training.

% Consider, for example, \ii{Alice said hello}, \ii{Bob said that Alice
%   said hello}, and \ii{Carl thinks that Bob said that Alice said
%   hello}. Overt recursion of this kind is easy to find, and
% theoretical accounts of natural language syntax and semantics rely
% heavily on recursive structures. In order for a model to be able to
% accurately learn natural language meanings, then, we expect that it
% would need to be able to learn to represent the meanings of function
% words in a such a way that they are able to behave correctly when
% taking their own outputs as input. In evaluating the model, we take
% advantage of the fact that recursive structures of this kind define
% potentially infinite languages by testing the model on strings that
% are longer and more complex than any seen in testing.

\paragraph{Experiments}
As in \S\ref{sec:join}, we test this phenomenon within the
framework of natural logic, but we now replace the unanalyzed symbols
from that experiment with complex formulae. These formulae
represent a truth-functionally complete classical propositional logic:
each atomic symbol is a variable over the domain \{$\True$, $\False$\}, and the only
operators are truth-functional ones.  Table~\ref{tab:pl} defines this
logic, and Table~\ref{tab:plexs} gives some examples of relational
statements that we can make in these terms. To compute these relations
between statements, we exhaustively enumerate the sets of assignments
of truth values to propositional variables that would satisfy each of
the statements, and then we convert the set-theoretic relation between
those assignments into one of the seven relations in
Table~\ref{b-table}. As a result, each relational statement represents
a valid theorem of the propositional logic.

\begin{table}[tp]
  \centering\small
  \begin{subtable}[t]{0.45\textwidth}
    \centering
    \begin{tabular}[t]{l l}
      \toprule
      Formula     & Interpretation \\
      \midrule
      $a$, $b$, $c$, $d$, $e$, $f$ & $\sem{x} \in \{\True, \False\}$ \\
      $\plneg \varphi$ & $\True$ iff $\sem{\varphi} = \False$ \\
      $(\varphi \pland \psi)$ & $\True$ iff $\False \notin \{\sem{\varphi}, \sem{\psi}\}$ \\
      $(\varphi \plor \psi)$  & $\True$ iff $\True \in \{\sem{\varphi}, \sem{\psi}\}$ \\
      \bottomrule
    \end{tabular}    
    \caption{Well-formed formulae. $\varphi$ and $\psi$
      range over all well-formed formulae, and $\sem{\cdot}$ is
      the interpretation function mapping formulae into $\{\True,
      \False\}$.}\label{tab:pl}
  \end{subtable}
  \begin{subtable}[t]{0.45\textwidth}
    \centering\vspace{4mm}
    \begin{tabular}[t]{r c l}
      \toprule
      $\plneg a$        & $\natneg$ & $a$ \\
      $\plneg \plneg a$ & $\nateq$  & $a$ \\
      $a$               & $\natfor$ & $(a \plor b)$ \\
      $(a \plor (b \plor c))$               & $\natrev$ & $(b \pland  \plneg c)$ \\
      %$(a \natfor b)$   & $\nateq$  & $(b \natrev a)$ \\	
      $\plneg\, (\plneg a \pland \plneg b)$ & $\nateq$ & $(a \plor b)$ \\ 
      \bottomrule
    \end{tabular}
    \caption{Examples of the type of statements used for training and testing. These are relations between
      well-formed formulae, computed in terms of sets of satisfying
      interpretation functions $\sem{\cdot}$.}\label{tab:plexs}
  \end{subtable}
  \caption{Natural logic relations over sentences of propositional logic.}  
  \label{prop-figure}
\end{table}

Socher et al.~\shortcite{socher2012semantic} show that a matrix-vector RNN
model somewhat similar to our RNTN can learn boolean logic, 
a logic where the atomic symbols are simply the
values $\True$ and $\False$. While learning the operators of that logic is not trivial, the outputs of
each operator can be represented accurately by a single bit.
In the more demainding task presented here, the atomic symbols are variables over these values, and the sentence vectors must thus be able to distinguish up to $2^{64}$ distinct conditions on valuations. Relational expressions between these conditions are essentially theorems of propositional logic, and to succeed, the model must learn to approximate the behavior of a theorem prover.

In our experiments, we randomly generate unique pairs 
of formulae and compute the relation that holds for each pair.
We discard pairs in which either statement is either a tautology or a
contradiction, for which the seven relations in
Table~\ref{b-table} are undefined. The resulting set of formula pairs is
then partitioned into 13 bins according the number of logical operators in
the longer of the two formulae. We then sample 20\% of each
bin for a held-out test set.

If we do not implement any constraint that the two statements being
compared are similar in any way, then the generated data are dominated
by statements in which the two formulae refer to largely separate
subsets of the six variables, which means that the $\natind$ relation
is almost always correct.  In an effort to balance the distribution of
relation labels without departing from the basic task of modeling
propositional logic, we disallow individual pairs of statements from
referring to more than four of the six propositional variables.

In order to test the model's generalization to unseen structures, we discard
training examples with more than 4 connectives, yielding 60k short training examples,
and 21k training examples containing up to 12 connectives.
We evaluated both the RNN and RNTN\footnote{We did not run any strictly simpler baselines for this experiment because it is demonstrably impossible to perform substantially above chance on longer expressions without word order information.} on the resulting data, initializing the model parameters randomly,
including the vector representations of the six variables.


\begin{figure*}[t]
  \centering
  \includegraphics[width=5.75in]{decayfig.eps}
%  \includegraphics[width=5.3in]{recursion\string_pairwise.eps}
  \caption{Model performance on propositional logic, by expression size.}
%    The dashed line indicates that only expressions of size four or less appeared in the training data. \textbf{Bottom:} Semantically distinct formulae should have different
%    representations. As measured by Euclidean distance, only the RNTN
%    achieves this for formulae containing more than a small number of
%    connectives (\ii{and} in this example). The RNN quickly collapses the representations,
%    failing to capture the meaning contrasts.
    
  \label{prop-results} 
  
\end{figure*}


\paragraph{Results} Figure~\ref{prop-results} shows test the relationship
between test accuracy and statement length. We found that both 
models were able to perform well on unseen small test examples, 
with RNN accuracy above
98\% and RNTN accuracy above 99\% on formulae below length five.
Starting at size four, performance gradually falls with increasing
size. Training accuracy was 99.4\% for the RNN and 99.8\% for the RNTN.

The performance of these models on small unseen test examples
indicates that they learned correct approximations of the underlying
logic. These approximations are accurate enough to yield
correct answers when the composition layer is only applied a small
number of times, but that the error in the approximation grows with
increasing expression size, resulting in gradually dropping performance. 
It appears that the RNTN model overfit somewhat relative to the plain RNN,
performing better on the training data and short examples, but worse on longer
examples. Two factors are likely involved in this: even with the lower dimension,
the RNTN composition function has about eight times as many parameters as the
RNN, and thus more capacity to find a solution involving a large number of narrow 
generalizations that work on the training data but don't generalize well. In addition,
the RNTN used here has slightly weaker L2 regularization than the 
RNN ($\lambda_{\textit{RNN}} = 0.001$, $\lambda_{\textit{RNTN}} = 0.0003$). This
yielded better average performance, but may have facilitated this effect.

% \mynote{In an effort to better understand why model performance decays, 
%we evaluated both models on pairs of long formulae involving
%binary connectives, to assess how well they distinguish representations for semantically distinct
%formulae. We found that only the RNTN is able to learn
%substantially different representations for pairs of differing formulae like $(a \pland (a \pland a))$
% and $(a \pland (a \pland b))$ when the difference between the two is placed under multiple operators.
%The RNN separates the bare symbols by a euclidean distance of 3.5, but this falls to less than one once the two are placed under three operators, and quickly approaches zero as depth increases.
% The RNTN separates the bare symbols by 2.5, and this falls off much more gradually to 1.3 with twelve operators.}

% we discovered that this model looses information about in longer
% statements in a particularly problematic way. It appears that that
% model is unable to distinguish between two sentences when the only
% difference between those sentences is embedded within a very deep
% structure. We evaluated both models on sentences that differed in only
% one term, but for which that one term was embedded under a large
% number of conjunctions, such as the pair \ii{a (and a)} and \ii{a (and
%   b)}, or the pair \ii{a (and (a (and a)))} and \ii{a (and (a (and
%   b)))}. We then measured the Euclidean distance between the vector
% representations of the two sentences in each pair. Our findings are
% shown in Figure~\ref{prop-falloff}, and show that while the RNTN can
% distinguish the two sentences well at every size that we test, the RNN
% fails after a depth of approximately six.


