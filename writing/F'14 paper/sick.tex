\section{The SICK textual entailment challenge}\label{sec:sick}

The specific model architecture that we use is novel, and though the underlying tree structure approach has been validated elsewhere, there is no guarantee that this architecture is suitable for handling inference in the face of the noisy labels and the diverse range of linguistic structures seen in typical natural language data. To investigate the ability of the model to learn on natural data, we train a version of our model on the SICK textual entailment challenge corpus \cite{marelli2014sick}. The corpus consists of about 10k natural language sentence pairs, labeled with \ii{entailment}, \ii{contradiction}, or \ii{neutral}. At only a few thousand distinct sentences (many of them variants on an even smaller set of template sentences), the corpus is not large enough to train a high quality learned model of general natural language, but it is the largest human-labeled entailment corpus that we are aware of, it is sufficient to provide some assurances as to the strength of these models.

Adapting to this task required us to make a few additions to the techniques discussed in \S\ref{methods}. In order to better handle rare words, we initialized our word embeddings using 200 dimensional vectors trained with 
GloVe \cite{pennington2014glove} on data from Wikipedia. Since 200 dimensional vectors are too large to be practical in an TreeRNTN on a small dataset, a new embedding transformation layer is needed. Before any embedding is used as an input to a recursive layer, it is passed through an additional $\tanh$ neural network layer with the same output dimension as the recursive layer. This new layer aggregates any usable information from the embedding vectors into a more compact working representation. An identical layer is added to the SumNN between the word vectors and the comparison layer.

We also supplemented the SICK training data\footnote{We tuned the model using performance on a held out development set, but report performance here for a version of the model trained on both the training and development data and tested on the SICK test set. We also report training accuracy on a small sample from each data source.} with 600k examples of entailment data from the Denotation Graph project (DG, \citealt{hodoshimage}, also used by the winning SICK submission), a corpus of noisy automatically labeled entailment examples over image captions, the same genre of text from which SICK was drawn. We trained a single model on data from both sources, but used a separate set of softmax parameters for classifying into the labels from each source. We parsed the data from both sources with the Stanford PCFG Parser v. 3.3.1 \cite{klein2003accurate}. We also found that we were able to train a working model much more quickly with an additional technique: we collapse subtrees that were identical across both sentences in a pair down to a single head word. The training and test data on which we report performance are collapsed in this way, and both collapsed and uncollapsed copies of the training data are used in training. Finally, in order to improve regularization on the noisier data, we used dropout \cite{srivastava2014dropout} at the input to the comparison layer (10\%) and at the output from the embedding transform layer (25\%). 

\begin{table}[tp]
  \centering \small
    \begin{tabular}{ l@{~~} r@{~~~~} r@{~~~~} r@{~~~~} r@{~~~~} }
    \toprule
        ~&\ii{neut.}&	 30d  & 			30d & 50d\\
    ~&only &SumNN  &TrRNN &TrRNTN\\ 
     \midrule
    DG Train	& 50.0 & 61.0? & 67.0 & \textbf{74.0} \\
    SICK Train	& 56.7 & 95.8? & 95.4 & \textbf{97.8} \\
    SICK Test	& 56.7 & 74.1? & 74.9 & \textbf{76.9} \\
    \midrule
    \textsc{Pass} (4\%)	& 7 & 73? & \textbf{93} & 87 \\   
    \textsc{Neg} (7\%)	& 5 & \textbf{100?} & 95 &75 \\
    \textsc{Sub} (24\%)	& 35 & 73? & 73 & \textbf{76} \\
    \textsc{Mult} (40\%)	& 76 & 66? & \textbf{78} &76 \\
    \textsc{Diff} (25\%)	& \textbf{95} & 86? & 82 & 91 \\  
    \midrule
    \textsc{Lt10} (47\%)	& 50.0 & 75.2? & 77.0 &\textbf{77.9} \\    
    \bottomrule
  \end{tabular}
  \caption{Classification accuracy, including a category breakdown for SICK test data. Categories are shown with their frequencies.}
  \label{sresultstable}
\end{table} 


% TODO: Update baseline results, apply bolding


\begin{table*}[htp]
  \centering\small
  \begin{tabular}{lcl}
    \toprule
    A man is cutting a paper plate	& \ii{entailment} & A paper plate is being cut by the man (\textsc{Pass})\\
    A little girl is playing the violin on a beach & \ii{contradiction} &	There is no girl playing the violin on a beach (\textsc{Neg})\\
    A man is fixing a silencer to a gun & \ii{entailment} & A man is fitting a silencer to a weapon (\textsc{Sub})\\
        A woman is breaking two eggs in a bowl & \ii{neutral} &A man is mixing a few ingredients in a bowl (\textsc{Multi})\\
        Dough is being spread by a man & \ii{neutral} & A woman is slicing meat with a knife (\textsc{Diff})\\
    \bottomrule
  \end{tabular}
  \caption{\label{examplesofsickdata}Examples of each category used in error analysis from the SICK test data. }
\end{table*}


\paragraph{Results} Despite the small amount of high quality training data available and the lack of resources for learning lexical relationships, the results (Table~\ref{sresultstable}) show that it is possible to train our model to perform competitively on textual entailment. Our TreeRNTN did not reach the performance of the winning system (84.6\%), but it did exceed that of eight out of 18 submitted systems, including several which used sophisticated hand-engineered features and lexical resources specific to the version of the entailment task at hand. 

% TODO: Update results

% TODO: Update analysis

It is often the case for textual entailment (and is known to be the case for SICK; \citealt{marelli2014semeval}), it is possible to perform well without making any attempt to take advantage of compositional syntactic or semantic structure, and our summing baseline model is powerful enough to do this. Our tree models nonetheless perform substantially better than this model, and we remain confident that given sufficient data, it should be possible for the tree models, and not the summing model, to learn a truly high-quality solution.

To better understand how the models differ, we manually annotated a fraction of the SICK test set, using mutually exclusive categories for passive/active alternation pairs (\textsc{Pass}), pairs differing only by the presence of negation (\textsc{Neg}), pairs differing by a single word or phrase substitution (\textsc{Sub}), pairs differing by multiple edits (\textsc{Mult}), and pairs with little or no content word overlap (\textsc{Diff}). Examples of each are in Table \ref{examplesofsickdata}. We annotated 100 random examples to judge the frequency of each category, and  continued selectively annotating until each categogy contained at least 25. We also use category (\textsc{Lt10}) for pairs in which neither sentence contains more than ten words.
 
The results (Table \ref{examplesofsickdata}) show that tree models perform well in the two categories which most narrowly pick out a specific syntactic structure, \textsc{Pass} and \textsc{Neg}. The baseline model also performs well on \textsc{Neg}, where identifying the presence of negation in only one sentence is sufficient to assign \ii{contradiction}, but it does not do as well on P/A, for which distinguishing correct entailments from distractors requires identifying that the same phrase describes fills different syntactic roles in the two sentences. 