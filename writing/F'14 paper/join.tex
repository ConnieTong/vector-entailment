\begin{table}[htp]
  \centering  \small
  \setlength{\arraycolsep}{8pt}
  \renewcommand{\arraystretch}{1.1}
  \newcommand{\UNK}{\cdot}  
  $\begin{array}[t]{c@{ \ }|*{7}{c}|}
    %\hline
    \multicolumn{1}{c}{}
             & \nateq     & \natfor     & \natrev     & \natneg    & \natalt     & \natcov     & \multicolumn{1}{c}{\natind} \\
    \cline{2-8}
    \nateq  & \nateq &   \natfor &  \natrev &  \natneg &   \natalt &  \natcov &  \natind \\
    \natfor & \natfor &  \natfor &  \UNK &  \natalt &   \natalt &  \UNK &  \UNK \\
    \natrev & \natrev &  \UNK &  \natrev &  \natcov &   \UNK &  \natcov &  \UNK \\
    \natneg & \natneg &  \natcov &  \natalt &  \nateq &    \natrev &  \natfor &  \natind \\
    \natalt & \natalt &  \UNK &  \natalt &  \natfor &   \UNK &  \natfor &  \UNK \\
    \natcov & \natcov &  \natcov &  \UNK &  \natrev &   \natrev &  \UNK &  \UNK \\
    \natind & \natind & \UNK &  \UNK &  \natind &  \UNK &  \UNK &  \UNK \\
    \cline{2-8}
  \end{array}$
  \caption{Inference path from premises $a\,R\,b$ (row) and $b\,S\,c$ (column) to the relation that holds between $a$ and $c$, if any.  These inferences are based on basic set-theoretic truths about the meanings of the underlying relations as described in Table~\ref{b-table}. We assess our models' ability to reproduce such inferential paths. Cells containing a dot correspond to pairs
of relations for which no valid inference can be drawn.}
  \label{tab:jointable}
\end{table}


\section{Reasoning about semantic relations}\label{sec:join}

If a model is to learn the behavior of a relational logic like the one
presented here from a finite amount of data, it must learn to deduce new
relations from seen relations in a sound manner. The simplest such
deductions involve atomic statements using the relations in
Table~\ref{b-table}. For instance, given that $a \natrev b$ and $b
\natrev c$, one can conclude that $a \natrev c$, by basic
set-theoretic reasoning (transitivity of $\natrev$). Similarly, from
$a \natfor b$ and $b \natneg c$, it follows that $a \natalt c$.  The
full set of sound such inferences is summarized in
Table~\ref{tab:jointable}.

% about the relations themselves that do not depend on the
% internal structure of the things being compared. For example, given
% that $a\sqsupset b$ and $b\sqsupset c$ one can conclude that
% $a\sqsupset c$ by the transitivity of $\sqsupset$, even without
% understanding $a$, $b$, or $c$. These seven relations support more
% than just transitivity: MacCartney and Manning's
% \cite{maccartney2009extended} join table defines 32 valid inferences
% that can be made on the basis of pairs of relations of the form $a R
% b$ and $b R' c$, including several less intuitive ones such as that if
% $a \natneg b$ and $b~|~c$ then $a \sqsupset c$.

\paragraph{Experiments}
To test our models' ability to learn these inferential patterns, we
create small boolean structures for our logic in which terms denote
sets of entities from a small domain.  Figure~\ref{lattice-figure}
depicts a structure of this form. The lattice gives the full model,
for which all the statements below it are valid. We divide these
statements evenly into training and test sets, and remove from the
test set statements which cannot be proven from the training
statements.
% using the logic depicted in Figure~\ref{lattice-figure}.

In each experimental run, we create 80 randomly generated sets drawing from
a domain of seven elements. This yields a data set consisting of
6400 statements about pairs of formulae. 3200 of these pairs are
chosen as a test set, and that test set is further reduced to the 2960
examples that can be provably derived from the training data.

We trained versions of both the RNN model and the RNTN model on these
data sets\footnote{Since this task relies entirely on the learning of the two entity vectors, no obvious comparable baseline presents itself.}. In both cases, the models are implemented as
described in Section~\ref{methods}, but since the items being compared
are single terms rather than full tree structures, the composition
layer is not involved, and the two models are not recursive and differ only in which
layer function is used for the comparison layer. We simply present
the models with the (randomly initialized) embedding vectors for each
of two terms, ensuring that the model has no information about the terms
being compared except for the relations between them that appear in training.


\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \newcommand{\labelednode}[4]{\put(#1,#2){\oval(1.5,1)}\put(#1,#2){\makebox(0,0){$\begin{array}{c}#3\\\{#4\}\end{array}$}}}
    \setlength{\unitlength}{1cm}\scalebox{0.8}{
    \begin{picture}(5,5.5)
      \labelednode{2.50}{5}{}{1,2,3}
      
      \put(0.75,4){\line(3,1){1.5}}
      \put(2.5,4){\line(0,1){0.5}}
      \put(4.25,4){\line(-3,1){1.5}}
      
      \labelednode{0.75}{3.5}{a,b}{1,2}
      \labelednode{2.50}{3.5}{c}{1,3}
      \labelednode{4.25}{3.5}{d}{2,3}
      
      \put(0.75,2.5){\line(0,1){0.5}}
      \put(0.75,2.5){\line(3,1){1.5}}
      
      \put(2.5,2.5){\line(-3,1){1.5}}
      \put(2.5,2.5){\line(3,1){1.5}}
      
      \put(4.25,2.5){\line(0,1){0.5}}
      \put(4.25,2.5){\line(-3,1){1.5}}
      

      \labelednode{0.75}{2}{e,f}{1}
      \labelednode{2.50}{2}{}{2}
      \labelednode{4.25}{2}{g,h}{3}
      
      \put(2.5,1){\line(-3,1){1.5}}
      \put(2.5,1){\line(0,1){0.5}}
      \put(2.5,1){\line(3,1){1.5}}
      
      \labelednode{2.5}{0.5}{}{}
    \end{picture}}
    \caption{Simple boolean structure. The letters name the sets. Not all sets have names, and
    some sets have multiple names, so that learning $\nateq$ is non-trivial.}
  \end{subfigure}
  \qquad
    \begin{subfigure}[t]{0.43\textwidth}
    \centering \vspace{0.4cm}
    \setlength{\tabcolsep}{12pt}
    \begin{tabular}[b]{c  c}
      \toprule
      Train & Test \\
      \midrule

      $a \nateq b$              & $b \natneg g$ \\
      $a \natrev e$              & $b \natrev e$ \\
      $d \natrev h$              & \strikeout{$e \nateq f$} \\
       $e \natalt g$              & \strikeout{$g \natfor d$} \\
      $g \natneg a$           & $h \natfor d$ \\

      \bottomrule
    \end{tabular}

    \caption{A few examples of atomic statements about the
      model.  Test statements that are not provable from the training data shown are
      crossed out.}
  \end{subfigure}  
  \caption{Small example structure and data for learning relation composition.}
  \label{lattice-figure}
\end{figure} 

% Note: None of these test examples is derivable from the shown training data, 
% but we suggest that there is additional training data, so we can cross lines
% out willy-nilly without fear.
\begin{table}[tp]
  \centering \small
  \begin{tabular}{ l r@{ \ }r r@{ \ }r r@{ \ }r }
    \toprule
    ~&\multicolumn{2}{c}{$\natind$ only} & \multicolumn{2}{c}{15d RNN}  & \multicolumn{2}{c}{15d RNTN}\\
    \midrule
    Train &53.8 &(10.5)    & 99.8&	(99.0) & \textbf{100} & \textbf{(100)}\\
    Test &52.0 &(10.3) &	94.0&(87.0)& \textbf{99.6} & \textbf{(95.5)}\\
    \bottomrule
  \end{tabular}
  \caption{Performance on the semantic relation experiments. These results and all other results on artificial data are reported as mean accuracy scores over five runs followed by mean macroaveraged F1 scores in parentheses.}
  \label{joinresultstable}
\end{table}

\paragraph{Results} 
Table \ref{joinresultstable} shows the results. 
The RNTN is able to accurately encode the relations 
between the terms in the geometric relations between their vectors, 
and is able to then use that information to recover relations that 
are not overtly included in the training data. The RNN also generalizes fairly well, 
but makes enough errors that it remains an open question whether 
it is capable of learning representations with these properties. 
We cannot rule out the possibility that different optimization techniques or
further hyperparameter tuning could lead an RNN model to succeed here.


